{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats as ss\n",
    "from scipy.sparse import csr_matrix, hstack, vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, Imputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import average_precision_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split, learning_curve, ShuffleSplit, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, NearMiss, InstanceHardnessThreshold\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция обработки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция для преобразования данных\n",
    "# num_cols - числовые признаки, которые будут использоваться\n",
    "# cat_cols - категориальные признаки, которые будут использоваться\n",
    "# опции: is_OHE - One Hot Encoding, \n",
    "# is_OverSampling - Over Sampling\n",
    "# imput_method - способ заполнения пропущенных числовых данных\n",
    "# feature_selection - метод отбора признаков \n",
    "\n",
    "# По умолчанию используется все признаки, все опции выключены, imput_method='normalize'\n",
    "# Для обработки категориальных данных по умолчанию используется только Label Encoder\n",
    "\n",
    "def transform_data(data, y=None, imput_method='normalize', feature_selection=None,\n",
    "                   num_cols=None, cat_cols=None, is_OHE=False, is_OverSampling=False):\n",
    "    \n",
    "    to_return = {}\n",
    "    \n",
    "    num_interval = (0, 190)\n",
    "    cat_interval = (191, data.shape[1])\n",
    "    \n",
    "    \n",
    "    data, num_interval, cat_interval, new_cols = drop_null_cols(data, num_interval, cat_interval)\n",
    "    \n",
    "    data_num, data_cat = split_num_cat(data, num_interval)\n",
    "    \n",
    "    if num_cols:\n",
    "        data_num = data[num_cols]\n",
    "        num_interval = (0, len(num_cols))\n",
    "        diff = 190 - len(num_cols)\n",
    "        cat_interval = (191 - diff, data.shape[1]-diff)\n",
    "        new_cols.drop(num_cols)\n",
    "    if cat_cols:\n",
    "        data_cat = data[cat_cols]\n",
    "        cat_interval_prev = cat_interval\n",
    "        cat_interval = (cat_interval_prev[0], cat_interval_prev[1]-len(cat_cols))\n",
    "        new_cols.drop(cat_cols)\n",
    "    \n",
    "    print('Processing the numeric columns...')\n",
    "    data_num = data_num_processing(data_num, method=imput_method)\n",
    "        \n",
    "    print('Processing the categorical columns...')\n",
    "    data_cat = data_cat_fill_nan(data_cat)\n",
    "    \n",
    "    data_cat = data_cat_LE(data_cat)\n",
    "    \n",
    "    if is_OHE:    \n",
    "        data_cat = data_cat_OHE(data_cat)\n",
    "        \n",
    "    data_result = data_join(data_num, data_cat).tocsr()\n",
    "    \n",
    "    if feature_selection:\n",
    "        data_result, model, feature_importances, feature_ranking = data_feature_selection(data=data_result, y=y, \n",
    "                                                                         method=feature_selection,\n",
    "                                                                         num_interval=num_interval,\n",
    "                                                                         columns=new_cols, is_OHE=is_OHE)\n",
    "                                                                         \n",
    "        to_return['feature_importances'] = feature_importances\n",
    "        to_return['model'] = model\n",
    "        to_return['feature_ranking']=feature_ranking\n",
    "        \n",
    "    \n",
    "    if is_OverSampling:\n",
    "        data_result, to_return['y'] = data_OverSampling(data_result, y)\n",
    "    \n",
    "    to_return['X'] = data_result\n",
    "    to_return['num_interval'] = num_interval\n",
    "    to_return['cat_interval'] = cat_interval\n",
    "        \n",
    "    print('Done!')\n",
    "    \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция удаления пустых колонок\n",
    "\n",
    "def drop_null_cols(data, num_interval, cat_interval):\n",
    "    \n",
    "    print('Dropping the null columns...')\n",
    "    dropped_num = []\n",
    "    dropped_cat = []\n",
    "    \n",
    "    new_cols = data.columns\n",
    "    \n",
    "    for idx, each_col in enumerate(data.columns):\n",
    "        if len(data[pd.isnull(data[each_col])==True])==data.shape[0]:\n",
    "            if idx > 190:\n",
    "                dropped_cat.append(each_col)\n",
    "            else:\n",
    "                dropped_num.append(each_col)\n",
    "                \n",
    "    for each_col in dropped_num:\n",
    "        data.drop(each_col,axis = 1,inplace = True)\n",
    "        print(\"Dropped num-type column: {0}\".format(each_col))\n",
    "    \n",
    "    for each_col in dropped_cat:\n",
    "        data.drop(each_col,axis = 1,inplace = True)\n",
    "        print(\"Dropped cal-type column: {0}\".format(each_col))\n",
    "    \n",
    "    max_num = 190 - len(dropped_num)\n",
    "    \n",
    "    print(\"Dropped {0} num-type cols and {1} cat-types cols\".format(len(dropped_num), len(dropped_cat)))\n",
    "    print(\"Num-type feature have the indices from {0} up to {1}.\".format(0, max_num))\n",
    "    print(\"Cal-type feature have the indices from {0} up to {1}.\".format(max_num+1, data.shape[1]))\n",
    "    \n",
    "    new_cols = new_cols.drop(dropped_num).drop(dropped_cat)\n",
    "    \n",
    "    \n",
    "    return data, (0, max_num), (max_num+1, data.shape[1]), new_cols\n",
    "\n",
    "# Функция разделения числовых и категориальных признаков\n",
    "\n",
    "def split_num_cat(data, num_interval):\n",
    "    print('Splitting the numeric and categorical columns...')\n",
    "    data_num, data_cat = np.split(data, [num_interval[1]], axis=1)\n",
    "    return data_num, data_cat\n",
    "\n",
    "# Функция обработки числовых данных\n",
    "def data_num_processing(data_num, method='normalize'):\n",
    "    \n",
    "    if method == 'normalize':\n",
    "        print('Filling the absent values with the columns\\' means...')\n",
    "        data_num_transformed = data_num.fillna(data_num.mean())\n",
    "        print('Normalizing the data...')\n",
    "        for each_col in data_num_transformed.columns:\n",
    "            if data_num_transformed[each_col].std():\n",
    "                data_num_transformed[each_col]=(data_num_transformed[each_col]-data_num_transformed[each_col].mean()) / float(data_num_transformed[each_col].std())\n",
    "            else:\n",
    "                data_num_transformed[each_col]=data_num_transformed[each_col]-data_num_transformed[each_col].mean()\n",
    "    else:\n",
    "        imputer = Imputer(strategy=method)\n",
    "        columns = data_num.columns\n",
    "        data_num_transformed = pd.DataFrame(imputer.fit_transform(data_num), columns=columns)\n",
    "        \n",
    "    return data_num_transformed\n",
    "\n",
    "# Заполнение пустых ячеек категориальных признаков значением 'unknown'\n",
    "def data_cat_fill_nan(data_cat):\n",
    "    print('Filling the absent categorical values...')\n",
    "    data_cat_trasformed = data_cat.fillna('unknown')\n",
    "    return data_cat_trasformed\n",
    "\n",
    "# Применение Label Encoder к категориальным данным\n",
    "def data_cat_LE(data_cat):\n",
    "    print('Applying a LabelEncoder to the categorical columns...')\n",
    "    \n",
    "    data_cat = data_cat.apply(lambda x: x.astype(str))\n",
    "    data_cat_transformed = data_cat.apply(lambda x: x.astype('category'))\n",
    "    \n",
    "    MyLabelEncoder = defaultdict(LabelEncoder)\n",
    "    data_cat_transformed = data_cat_transformed.apply(lambda x: MyLabelEncoder[x.name].fit_transform(x))\n",
    "    \n",
    "    return data_cat_transformed\n",
    "\n",
    "# Применение One Hot Encoder к категориальным данным\n",
    "\n",
    "def data_cat_OHE(data_cat):\n",
    "    print('Applying a OneHotEncoder to categorical columns...')\n",
    "    data_cat_transformed = OneHotEncoder().fit_transform(data_cat)\n",
    "    return data_cat_transformed\n",
    "\n",
    "# Соединение обработанных числовых и категориальных признаков после обработки\n",
    "\n",
    "def data_join(data_num, data_cat):\n",
    "    print('Joining the numerical and categorical columns...')\n",
    "    data_num_sparsed = csr_matrix(data_num)\n",
    "    data_cat_sparsed = csr_matrix(data_cat)\n",
    "    \n",
    "    data_result = hstack((data_num_sparsed, data_cat))\n",
    "    \n",
    "    return data_result\n",
    "\n",
    "# OverSampling\n",
    "# Применимо только если есть значения labels\n",
    "\n",
    "def data_OverSampling(data, labels):\n",
    "    print('OverSampling in progress...')\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_num_cat_resampled, y_resampled = ros.fit_sample(data, labels['target'].ravel())\n",
    "    return X_num_cat_resampled, y_resampled\n",
    "\n",
    "# Отбор признаков\n",
    "# Доступны два метода: l1-регуляризация и отбор на основе деревьев\n",
    "def data_feature_selection(data, y, method, num_interval, columns, is_OHE):\n",
    "    print('Appling the {}-based feature selection method'.format(method))\n",
    "    if method == 'l1':\n",
    "        lsvc = LinearSVC(C=0.01, penalty='l1', dual=False).fit(data, y)\n",
    "        model = SelectFromModel(lsvc, prefit=True)\n",
    "        data_transformed = model.transform(data)\n",
    "        print('New shape of X-data is {}'.format(data_transformed.shape))\n",
    "        importances = lsvc.coef_\n",
    "    elif method == 'tree':\n",
    "        clf = ExtraTreesClassifier().fit(data, y)\n",
    "        model = SelectFromModel(clf, prefit=True)\n",
    "        data_transformed = model.transform(data)\n",
    "        print('New shape of X-data is {}'.format(data_transformed.shape))\n",
    "        importances = clf.feature_importances_\n",
    "        \n",
    "    if not is_OHE:\n",
    "        features_ranking = columns[importances.argsort()]\n",
    "    else:\n",
    "        indices = np.arange(num_interval[1])\n",
    "        num_importances = np.take(np.array(importances), indices)\n",
    "        features_ranking = columns[num_importances.argsort()]\n",
    "    \n",
    "    return data_transformed, model, importances, features_ranking\n",
    "\n",
    "# Функция расчета метрик отложенной выборки\n",
    "def get_scores_hold_out(X_hold_out, y_hold_out, Classifier, configuration, scores):\n",
    "    \n",
    "    print('Calculating scores...')\n",
    "    \n",
    "    y_proba = Classifier.predict_proba(X_hold_out)[:, 1]\n",
    "    y_pred = Classifier.predict(X_hold_out)\n",
    "\n",
    "    results = precision_recall_fscore_support(y_hold_out, y_pred)\n",
    "    pr_scores = average_precision_score(y_hold_out, y_proba)\n",
    "    roc_auc_scores = roc_auc_score(y_hold_out, y_proba)\n",
    "    \n",
    "    scores.loc[configuration, 'precision'] = results[0][1]\n",
    "    scores.loc[configuration, 'recall'] = results[1][1]\n",
    "    scores.loc[configuration, 'f1-score'] = results[2][1]\n",
    "    \n",
    "    scores.loc[configuration, 'PR-score'] = pr_scores\n",
    "    scores.loc[configuration, 'ROC AUC'] = roc_auc_scores\n",
    "    \n",
    "    print('Done!')\n",
    "    return True\n",
    "\n",
    "# Функция расчета метрик основной выборки\n",
    "def get_scores(X, y, Classifier, configuration, scores):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=123)\n",
    "    skf.get_n_splits(X, y)\n",
    "    skf.split(X, y)\n",
    "    \n",
    "    print('Calculating scores...')\n",
    "    results = cross_validate(Classifier, X, y, cv=skf, scoring=['precision', 'recall', 'f1', 'average_precision', 'roc_auc'], return_train_score=True)\n",
    "    \n",
    "    scores.loc[configuration, 'precision test'] = results['test_precision'].mean()\n",
    "    scores.loc[configuration, 'precision train'] = results['train_precision'].mean()\n",
    "    scores.loc[configuration, 'recall test'] = results['test_recall'].mean()\n",
    "    scores.loc[configuration, 'recall train'] = results['train_recall'].mean()\n",
    "    scores.loc[configuration, 'f1-score test'] = results['test_f1'].mean()\n",
    "    scores.loc[configuration, 'f1-score train'] = results['train_f1'].mean()\n",
    "    scores.loc[configuration, 'PR-score test'] = results['test_average_precision'].mean()\n",
    "    scores.loc[configuration, 'PR-score train'] = results['train_average_precision'].mean()\n",
    "    scores.loc[configuration, 'ROC AUC test'] = results['test_roc_auc'].mean()\n",
    "    scores.loc[configuration, 'ROC AUC train'] = results['train_roc_auc'].mean()\n",
    "    \n",
    "    print('Done!')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция отрисовки кривых обучения\n",
    "# Взято отсюда: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, scoring=None, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    \n",
    "    if scoring:\n",
    "        plt.ylabel(scoring)\n",
    "    else:\n",
    "        plt.ylabel('Score')\n",
    "    \n",
    "    if scoring:\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "        \n",
    "    else:\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "        \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция для изменения размера выборки с указанием доли положительного и отрицательного классов\n",
    "# pos_frac - доля объектов положительного класса начальной выборки, которые будут использоваться в итоговой выборке\n",
    "# neg_frac - доля объектов отрицательного класса начальной выборки, которые будут использоваться в итоговой выборке\n",
    "\n",
    "def resample_data(X, y, pos_frac, neg_frac):\n",
    "    \n",
    "    pos_size = int(len(np.where(y==1)[0]) * pos_frac)\n",
    "    neg_size = int(len(np.where(y==-1)[0]) * neg_frac)\n",
    "    \n",
    "    random.seed(1)\n",
    "    pos_indices = random.sample(list(np.where(y== 1)[0]), pos_size)\n",
    "    neg_indices = random.sample(list(np.where(y==-1)[0]), neg_size)\n",
    "    \n",
    "    y_pos = y[pos_indices]\n",
    "    y_neg = y[neg_indices]\n",
    "    y_result = np.append(y_pos, y_neg).ravel()\n",
    "    \n",
    "    X_pos = X[pos_indices]\n",
    "    X_neg = X[neg_indices]\n",
    "    X_result = vstack((X_pos, X_neg))\n",
    "    \n",
    "    pos_rate_result = X_result[y_result==1].shape[0] / float(X_result.shape[0])\n",
    "    neg_rate_result = X_result[y_result==-1].shape[0] / float(X_result.shape[0])\n",
    "    shape_result = X_result.shape[0]\n",
    "    \n",
    "    print('The new size is {0}, the positive and negative class fractions are {1:.2f} and {2:.2f}'.format(shape_result, pos_rate_result, neg_rate_result))\n",
    "\n",
    "    return X_result, y_result, pos_rate_result, neg_rate_result, shape_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Датафреймы с метриками различных конфигураций обработки данных и классификатора\n",
    "# для обучающей и отложенной выборок\n",
    "\n",
    "scores_cv = pd.DataFrame(columns = ['precision test', 'precision train', 'recall test', 'recall train', 'f1-score test', 'f1-score train','PR-score test','PR-score train', 'ROC AUC test', 'ROC AUC train'])\n",
    "scores_train = pd.DataFrame(columns = ['precision', 'recall', 'f1-score', 'PR-score', 'ROC AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Топ-20 числовых признаков по корреляции\n",
    "num_top_20 = ['Var188',\n",
    " 'Var130',\n",
    " 'Var189',\n",
    " 'Var114',\n",
    " 'Var73',\n",
    " 'Var111',\n",
    " 'Var126',\n",
    " 'Var7',\n",
    " 'Var177',\n",
    " 'Var168',\n",
    " 'Var139',\n",
    " 'Var53',\n",
    " 'Var142',\n",
    " 'Var92',\n",
    " 'Var144',\n",
    " 'Var147',\n",
    " 'Var69',\n",
    " 'Var136',\n",
    " 'Var110',\n",
    " 'Var51']\n",
    "\n",
    "# Топ-20 категориальных признаков по корреляции\n",
    "cat_top_20 = ['Var199',\n",
    " 'Var192',\n",
    " 'Var216',\n",
    " 'Var206',\n",
    " 'Var212',\n",
    " 'Var205',\n",
    " 'Var228',\n",
    " 'Var193',\n",
    " 'Var207',\n",
    " 'Var227',\n",
    " 'Var204',\n",
    " 'Var221',\n",
    " 'Var210',\n",
    " 'Var218',\n",
    " 'Var200',\n",
    " 'Var214',\n",
    " 'Var226',\n",
    " 'Var197',\n",
    " 'Var211',\n",
    " 'Var225']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "labels = pd.read_csv('orange_small_churn_labels.txt', header=None)\n",
    "labels.columns = ['target']\n",
    "labels = labels.apply(lambda x: x.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data_transformed = transform_data(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = labels.as_matrix().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-graded Assignment: Эксперименты с моделью\n",
    "\n",
    "На прошлой неделе вы поучаствовали в соревновании на kaggle и, наверняка, большинство успешно справилось с прохождением baseline, а значит пора двигаться дальше - заняться оптимизацией модели, провести серию экспериментов и построить сильное финальное решения.\n",
    "\n",
    "В этом задании вам нужно провести ряд эскпериментов, оценить качество полученных в процессе экспериментирования моделей и выбрать лучшее решение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание будет оцениваться на основании загруженного jupyther notebook и развернутых ответов на поставленные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Начнем с простого. Давайте оценим как много объектов действительно нужно для построения качественной модели. Для обучения доступна достаточно большая выборка и может так оказаться, что начиная с некоторого момента рост размера обучающей выборки перестает влиять на качество модели. Постройте кривые обучения, обучая модель на выборках разного размера начиная с небольшого количество объектов в обучающей выборке и постепенно наращивая её размер с некоторым шагом. Обратите внимание на `sklearn.model_selection.learning_curve`\n",
    "\n",
    "* Если не подвергать данные никакой другой обработке, то можно остановиться на выборке в 25к"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': <40000x212 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 8205377 stored elements in Compressed Sparse Row format>,\n",
       " 'cat_interval': (175, 212),\n",
       " 'num_interval': (0, 174)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/T8Qg1JLjHKPtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdab08d0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdaccda550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdabb7ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdabb3d748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, random_state=123)\n",
    "estimator = GradientBoostingClassifier()\n",
    "title = \"Learning Curves Gradient Boosting Classifier (ROC AUC)\"\n",
    "plot_learning_curve(estimator, title, scoring='roc_auc', X=data_transformed['X'], y=y, cv=cv, n_jobs=1)\n",
    "title = \"Learning Curves Gradient Boosting Classifier (PR-score)\"\n",
    "plot_learning_curve(estimator, title, scoring='average_precision', X=data_transformed['X'], y=y, cv=cv, n_jobs=1)\n",
    "title = \"Learning Curves Gradient Boosting Classifier (Recall)\"\n",
    "plot_learning_curve(estimator, title, scoring='recall', X=data_transformed['X'], y=y, cv=cv, n_jobs=1)\n",
    "title = \"Learning Curves Gradient Boosting Classifier (Precision)\"\n",
    "plot_learning_curve(estimator, title, scoring='precision', X=data_transformed['X'], y=y, cv=cv, n_jobs=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Часто несбалансированные по классам выборки приводят к различным проблемам при обучении моделей. Давайте попробуем по-разному обработать выборку, поиграть с распределением объектов по классам и сделать выводы о том, как соотношение классов влияет на качество модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1\\. Задайте веса объектам так, чтобы соотношение классов с учетом весов объектов изменилось. Попробуйте не менее трёх различных вариантов весов. Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?\n",
    "\n",
    "* С изменением доли класса \"отток\" метрики существенно изменяются\n",
    "* Метрики растут, модель улучшается\n",
    "* Оптимальным является модель, в которой доли классов \"отток\" и \"не отток\" приблизительно одинаковы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_frac = 0.68\n",
      "The new size is 28152, the positive and negative class fractions are 0.11 and 0.89\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "neg_frac = 0.3\n",
      "The new size is 14083, the positive and negative class fractions are 0.21 and 0.79\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "neg_frac = 0.16\n",
      "The new size is 8899, the positive and negative class fractions are 0.33 and 0.67\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "neg_frac = 0.11\n",
      "The new size is 7048, the positive and negative class fractions are 0.42 and 0.58\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "neg_frac = 0.09\n",
      "The new size is 6308, the positive and negative class fractions are 0.47 and 0.53\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "neg_frac = 0.075\n",
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "neg_frac = [0.68, 0.30, 0.16, 0.11, 0.09, 0.075]\n",
    "positive_fraction = []\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "for each_frac in neg_frac:\n",
    "    print('neg_frac = {0}'.format(each_frac))\n",
    "    X_1, y_1, pos_frac, _, _ = resample_data(data_transformed['X'], y, 1., each_frac)\n",
    "    configuration='Positive class fraction = ' + str(round(pos_frac, 2))\n",
    "    positive_fraction.append(pos_frac)\n",
    "    get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "    gbc.fit(X_1, y_1)\n",
    "    get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision test</th>\n",
       "      <th>precision train</th>\n",
       "      <th>recall test</th>\n",
       "      <th>recall train</th>\n",
       "      <th>f1-score test</th>\n",
       "      <th>f1-score train</th>\n",
       "      <th>PR-score test</th>\n",
       "      <th>PR-score train</th>\n",
       "      <th>ROC AUC test</th>\n",
       "      <th>ROC AUC train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.543336</td>\n",
       "      <td>0.802362</td>\n",
       "      <td>0.0302397</td>\n",
       "      <td>0.048051</td>\n",
       "      <td>0.0572347</td>\n",
       "      <td>0.0905694</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>0.37886</td>\n",
       "      <td>0.736245</td>\n",
       "      <td>0.7939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.580892</td>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.277421</td>\n",
       "      <td>0.432115</td>\n",
       "      <td>0.555641</td>\n",
       "      <td>0.734751</td>\n",
       "      <td>0.808143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.735712</td>\n",
       "      <td>0.346785</td>\n",
       "      <td>0.410786</td>\n",
       "      <td>0.45063</td>\n",
       "      <td>0.52707</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.693886</td>\n",
       "      <td>0.738533</td>\n",
       "      <td>0.81657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.651932</td>\n",
       "      <td>0.738259</td>\n",
       "      <td>0.52521</td>\n",
       "      <td>0.60988</td>\n",
       "      <td>0.581503</td>\n",
       "      <td>0.667917</td>\n",
       "      <td>0.658956</td>\n",
       "      <td>0.770807</td>\n",
       "      <td>0.735974</td>\n",
       "      <td>0.82293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.664977</td>\n",
       "      <td>0.740377</td>\n",
       "      <td>0.639449</td>\n",
       "      <td>0.711441</td>\n",
       "      <td>0.651841</td>\n",
       "      <td>0.725551</td>\n",
       "      <td>0.705328</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.736613</td>\n",
       "      <td>0.830574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.670118</td>\n",
       "      <td>0.74205</td>\n",
       "      <td>0.718073</td>\n",
       "      <td>0.793011</td>\n",
       "      <td>0.693176</td>\n",
       "      <td>0.766639</td>\n",
       "      <td>0.733713</td>\n",
       "      <td>0.839188</td>\n",
       "      <td>0.732077</td>\n",
       "      <td>0.835514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               precision test precision train recall test  \\\n",
       "Positive class fraction = 0.11       0.543336        0.802362   0.0302397   \n",
       "Positive class fraction = 0.21       0.580892          0.7348     0.13273   \n",
       "Positive class fraction = 0.33       0.645367        0.735712    0.346785   \n",
       "Positive class fraction = 0.42       0.651932        0.738259     0.52521   \n",
       "Positive class fraction = 0.47       0.664977        0.740377    0.639449   \n",
       "Positive class fraction = 0.52       0.670118         0.74205    0.718073   \n",
       "\n",
       "                               recall train f1-score test f1-score train  \\\n",
       "Positive class fraction = 0.11     0.048051     0.0572347      0.0905694   \n",
       "Positive class fraction = 0.21     0.171034      0.215669       0.277421   \n",
       "Positive class fraction = 0.33     0.410786       0.45063        0.52707   \n",
       "Positive class fraction = 0.42      0.60988      0.581503       0.667917   \n",
       "Positive class fraction = 0.47     0.711441      0.651841       0.725551   \n",
       "Positive class fraction = 0.52     0.793011      0.693176       0.766639   \n",
       "\n",
       "                               PR-score test PR-score train ROC AUC test  \\\n",
       "Positive class fraction = 0.11      0.267859        0.37886     0.736245   \n",
       "Positive class fraction = 0.21      0.432115       0.555641     0.734751   \n",
       "Positive class fraction = 0.33      0.580529       0.693886     0.738533   \n",
       "Positive class fraction = 0.42      0.658956       0.770807     0.735974   \n",
       "Positive class fraction = 0.47      0.705328       0.811321     0.736613   \n",
       "Positive class fraction = 0.52      0.733713       0.839188     0.732077   \n",
       "\n",
       "                               ROC AUC train  \n",
       "Positive class fraction = 0.11        0.7939  \n",
       "Positive class fraction = 0.21      0.808143  \n",
       "Positive class fraction = 0.33       0.81657  \n",
       "Positive class fraction = 0.42       0.82293  \n",
       "Positive class fraction = 0.47      0.830574  \n",
       "Positive class fraction = 0.52      0.835514  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>PR-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.766467</td>\n",
       "      <td>0.0430108</td>\n",
       "      <td>0.0814508</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.784093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.70901</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>0.262798</td>\n",
       "      <td>0.533307</td>\n",
       "      <td>0.797007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.721351</td>\n",
       "      <td>0.401882</td>\n",
       "      <td>0.516185</td>\n",
       "      <td>0.678596</td>\n",
       "      <td>0.806856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.72439</td>\n",
       "      <td>0.59879</td>\n",
       "      <td>0.655629</td>\n",
       "      <td>0.754367</td>\n",
       "      <td>0.812276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.728933</td>\n",
       "      <td>0.697581</td>\n",
       "      <td>0.712912</td>\n",
       "      <td>0.796059</td>\n",
       "      <td>0.817358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               precision     recall   f1-score  PR-score  \\\n",
       "Positive class fraction = 0.11  0.766467  0.0430108  0.0814508  0.356719   \n",
       "Positive class fraction = 0.21   0.70901    0.16129   0.262798  0.533307   \n",
       "Positive class fraction = 0.33  0.721351   0.401882   0.516185  0.678596   \n",
       "Positive class fraction = 0.42   0.72439    0.59879   0.655629  0.754367   \n",
       "Positive class fraction = 0.47  0.728933   0.697581   0.712912  0.796059   \n",
       "Positive class fraction = 0.52  0.731923   0.778898    0.75468  0.827556   \n",
       "\n",
       "                                 ROC AUC  \n",
       "Positive class fraction = 0.11  0.784093  \n",
       "Positive class fraction = 0.21  0.797007  \n",
       "Positive class fraction = 0.33  0.806856  \n",
       "Positive class fraction = 0.42  0.812276  \n",
       "Positive class fraction = 0.47  0.817358  \n",
       "Positive class fraction = 0.52  0.822084  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsf+xyv281WoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdaba6f390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRN8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdab9c7cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdab994cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZlby/1svbe5ZYr9qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdaba0b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbdab92b860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for each_score in scores_train.columns:\n",
    "    plt.figure()\n",
    "    \n",
    "    title = each_score + ' with different fractions of the positive class'\n",
    "    plt.title(title)\n",
    "    plt.xlabel('positive class fraction')\n",
    "    plt.ylabel(each_score)\n",
    "    plt.grid()\n",
    "    \n",
    "    train_col = each_score\n",
    "    cross_val_col = each_score + ' ' + 'test'\n",
    "    \n",
    "    plt.plot(positive_fraction, np.array(scores_train[train_col]), 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    plt.plot(positive_fraction, np.array(scores_cv[cross_val_col]), 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2\\. Примените к выборке технологию undersampling: для этого нужно убрать из обучения некоторое количество объектов большего класса таким образом, чтобы соотношение классов изменилось. Попробуйте не менее трёх различных вариантов undersampling (варианты могут отличаться как по количество отфильтрованных объектов, так и по принципу выборка объектов для отсеивания из выборки). Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?\n",
    "\n",
    "* RandomUnderSampler и InstanceHardnessThreshold - почти не меняются, NearMiss - меняются\n",
    "* RandomUnderSampler и InstanceHardnessThreshold - почти не изменяется, NearMiss - качетсво улучшается\n",
    "* NearMiss - оптимальный выбор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iht = InstanceHardnessThreshold(random_state=0)\n",
    "random_us = RandomUnderSampler(random_state=0)\n",
    "near_miss = NearMiss(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_resampled_rand, y_resampled_rand = random_us.fit_sample(data_transformed['X'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_resampled_near, y_resampled_near = near_miss.fit_sample(data_transformed['X'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_resampled_iht, y_resampled_iht = iht.fit_sample(data_transformed['X'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(X_resampled_rand, y_resampled_rand, gbc, 'RandomUnderSampler', scores_cv)\n",
    "gbc.fit(X_resampled_rand, y_resampled_rand)\n",
    "get_scores_hold_out(X_resampled_rand, y_resampled_rand, gbc, 'RandomUnderSampler', scores_train)\n",
    "\n",
    "get_scores(X_resampled_near, y_resampled_near, gbc, 'NearMiss UnderSampler', scores_cv)\n",
    "gbc.fit(X_resampled_near, y_resampled_near)\n",
    "get_scores_hold_out(X_resampled_near, y_resampled_near, gbc, 'NearMiss UnderSampler', scores_train)\n",
    "\n",
    "get_scores(X_resampled_iht, y_resampled_iht, gbc, 'InstanceHardnessThreshold UnderSampler', scores_cv)\n",
    "gbc.fit(X_resampled_iht, y_resampled_iht)\n",
    "get_scores_hold_out(X_resampled_iht, y_resampled_iht, gbc, 'InstanceHardnessThreshold UnderSampler', scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5952\n",
      "0.5\n",
      "5952\n",
      "0.5\n",
      "20617\n",
      "0.1443468981908134\n"
     ]
    }
   ],
   "source": [
    "print(X_resampled_rand.shape[0])\n",
    "print(X_resampled_rand[y_resampled_rand==1].shape[0] /float(X_resampled_rand.shape[0]))\n",
    "print(X_resampled_near.shape[0])\n",
    "print(X_resampled_near[y_resampled_near==1].shape[0] /float(X_resampled_near.shape[0]))\n",
    "print(X_resampled_iht.shape[0])\n",
    "print(X_resampled_iht[y_resampled_iht==1].shape[0] /float(X_resampled_iht.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision test</th>\n",
       "      <th>precision train</th>\n",
       "      <th>recall test</th>\n",
       "      <th>recall train</th>\n",
       "      <th>f1-score test</th>\n",
       "      <th>f1-score train</th>\n",
       "      <th>PR-score test</th>\n",
       "      <th>PR-score train</th>\n",
       "      <th>ROC AUC test</th>\n",
       "      <th>ROC AUC train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.543336</td>\n",
       "      <td>0.802362</td>\n",
       "      <td>0.0302397</td>\n",
       "      <td>0.048051</td>\n",
       "      <td>0.0572347</td>\n",
       "      <td>0.0905694</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>0.37886</td>\n",
       "      <td>0.736245</td>\n",
       "      <td>0.7939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.580892</td>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.277421</td>\n",
       "      <td>0.432115</td>\n",
       "      <td>0.555641</td>\n",
       "      <td>0.734751</td>\n",
       "      <td>0.808143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.735712</td>\n",
       "      <td>0.346785</td>\n",
       "      <td>0.410786</td>\n",
       "      <td>0.45063</td>\n",
       "      <td>0.52707</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.693886</td>\n",
       "      <td>0.738533</td>\n",
       "      <td>0.81657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.651932</td>\n",
       "      <td>0.738259</td>\n",
       "      <td>0.52521</td>\n",
       "      <td>0.60988</td>\n",
       "      <td>0.581503</td>\n",
       "      <td>0.667917</td>\n",
       "      <td>0.658956</td>\n",
       "      <td>0.770807</td>\n",
       "      <td>0.735974</td>\n",
       "      <td>0.82293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.664977</td>\n",
       "      <td>0.740377</td>\n",
       "      <td>0.639449</td>\n",
       "      <td>0.711441</td>\n",
       "      <td>0.651841</td>\n",
       "      <td>0.725551</td>\n",
       "      <td>0.705328</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.736613</td>\n",
       "      <td>0.830574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.670118</td>\n",
       "      <td>0.74205</td>\n",
       "      <td>0.718073</td>\n",
       "      <td>0.793011</td>\n",
       "      <td>0.693176</td>\n",
       "      <td>0.766639</td>\n",
       "      <td>0.733713</td>\n",
       "      <td>0.839188</td>\n",
       "      <td>0.732077</td>\n",
       "      <td>0.835514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.667147</td>\n",
       "      <td>0.734895</td>\n",
       "      <td>0.694553</td>\n",
       "      <td>0.767053</td>\n",
       "      <td>0.68044</td>\n",
       "      <td>0.750612</td>\n",
       "      <td>0.724784</td>\n",
       "      <td>0.823841</td>\n",
       "      <td>0.735272</td>\n",
       "      <td>0.830512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.824846</td>\n",
       "      <td>0.897813</td>\n",
       "      <td>0.67541</td>\n",
       "      <td>0.743784</td>\n",
       "      <td>0.742652</td>\n",
       "      <td>0.813564</td>\n",
       "      <td>0.877452</td>\n",
       "      <td>0.929137</td>\n",
       "      <td>0.852617</td>\n",
       "      <td>0.915934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.686542</td>\n",
       "      <td>0.767378</td>\n",
       "      <td>0.257055</td>\n",
       "      <td>0.291751</td>\n",
       "      <td>0.373709</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.512349</td>\n",
       "      <td>0.598667</td>\n",
       "      <td>0.816627</td>\n",
       "      <td>0.858223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       precision test precision train  \\\n",
       "Positive class fraction = 0.11               0.543336        0.802362   \n",
       "Positive class fraction = 0.21               0.580892          0.7348   \n",
       "Positive class fraction = 0.33               0.645367        0.735712   \n",
       "Positive class fraction = 0.42               0.651932        0.738259   \n",
       "Positive class fraction = 0.47               0.664977        0.740377   \n",
       "Positive class fraction = 0.52               0.670118         0.74205   \n",
       "RandomUnderSampler                           0.667147        0.734895   \n",
       "NearMiss UnderSampler                        0.824846        0.897813   \n",
       "InstanceHardnessThreshold UnderSampler       0.686542        0.767378   \n",
       "\n",
       "                                       recall test recall train f1-score test  \\\n",
       "Positive class fraction = 0.11           0.0302397     0.048051     0.0572347   \n",
       "Positive class fraction = 0.21             0.13273     0.171034      0.215669   \n",
       "Positive class fraction = 0.33            0.346785     0.410786       0.45063   \n",
       "Positive class fraction = 0.42             0.52521      0.60988      0.581503   \n",
       "Positive class fraction = 0.47            0.639449     0.711441      0.651841   \n",
       "Positive class fraction = 0.52            0.718073     0.793011      0.693176   \n",
       "RandomUnderSampler                        0.694553     0.767053       0.68044   \n",
       "NearMiss UnderSampler                      0.67541     0.743784      0.742652   \n",
       "InstanceHardnessThreshold UnderSampler    0.257055     0.291751      0.373709   \n",
       "\n",
       "                                       f1-score train PR-score test  \\\n",
       "Positive class fraction = 0.11              0.0905694      0.267859   \n",
       "Positive class fraction = 0.21               0.277421      0.432115   \n",
       "Positive class fraction = 0.33                0.52707      0.580529   \n",
       "Positive class fraction = 0.42               0.667917      0.658956   \n",
       "Positive class fraction = 0.47               0.725551      0.705328   \n",
       "Positive class fraction = 0.52               0.766639      0.733713   \n",
       "RandomUnderSampler                           0.750612      0.724784   \n",
       "NearMiss UnderSampler                        0.813564      0.877452   \n",
       "InstanceHardnessThreshold UnderSampler       0.422743      0.512349   \n",
       "\n",
       "                                       PR-score train ROC AUC test  \\\n",
       "Positive class fraction = 0.11                0.37886     0.736245   \n",
       "Positive class fraction = 0.21               0.555641     0.734751   \n",
       "Positive class fraction = 0.33               0.693886     0.738533   \n",
       "Positive class fraction = 0.42               0.770807     0.735974   \n",
       "Positive class fraction = 0.47               0.811321     0.736613   \n",
       "Positive class fraction = 0.52               0.839188     0.732077   \n",
       "RandomUnderSampler                           0.823841     0.735272   \n",
       "NearMiss UnderSampler                        0.929137     0.852617   \n",
       "InstanceHardnessThreshold UnderSampler       0.598667     0.816627   \n",
       "\n",
       "                                       ROC AUC train  \n",
       "Positive class fraction = 0.11                0.7939  \n",
       "Positive class fraction = 0.21              0.808143  \n",
       "Positive class fraction = 0.33               0.81657  \n",
       "Positive class fraction = 0.42               0.82293  \n",
       "Positive class fraction = 0.47              0.830574  \n",
       "Positive class fraction = 0.52              0.835514  \n",
       "RandomUnderSampler                          0.830512  \n",
       "NearMiss UnderSampler                       0.915934  \n",
       "InstanceHardnessThreshold UnderSampler      0.858223  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>PR-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.766467</td>\n",
       "      <td>0.0430108</td>\n",
       "      <td>0.0814508</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.784093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.70901</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>0.262798</td>\n",
       "      <td>0.533307</td>\n",
       "      <td>0.797007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.721351</td>\n",
       "      <td>0.401882</td>\n",
       "      <td>0.516185</td>\n",
       "      <td>0.678596</td>\n",
       "      <td>0.806856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.72439</td>\n",
       "      <td>0.59879</td>\n",
       "      <td>0.655629</td>\n",
       "      <td>0.754367</td>\n",
       "      <td>0.812276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.728933</td>\n",
       "      <td>0.697581</td>\n",
       "      <td>0.712912</td>\n",
       "      <td>0.796059</td>\n",
       "      <td>0.817358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.721953</td>\n",
       "      <td>0.750336</td>\n",
       "      <td>0.735871</td>\n",
       "      <td>0.810379</td>\n",
       "      <td>0.817704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.884412</td>\n",
       "      <td>0.730175</td>\n",
       "      <td>0.799926</td>\n",
       "      <td>0.921433</td>\n",
       "      <td>0.906948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.755157</td>\n",
       "      <td>0.28293</td>\n",
       "      <td>0.411635</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.853209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       precision     recall   f1-score  \\\n",
       "Positive class fraction = 0.11          0.766467  0.0430108  0.0814508   \n",
       "Positive class fraction = 0.21           0.70901    0.16129   0.262798   \n",
       "Positive class fraction = 0.33          0.721351   0.401882   0.516185   \n",
       "Positive class fraction = 0.42           0.72439    0.59879   0.655629   \n",
       "Positive class fraction = 0.47          0.728933   0.697581   0.712912   \n",
       "Positive class fraction = 0.52          0.731923   0.778898    0.75468   \n",
       "RandomUnderSampler                      0.721953   0.750336   0.735871   \n",
       "NearMiss UnderSampler                   0.884412   0.730175   0.799926   \n",
       "InstanceHardnessThreshold UnderSampler  0.755157    0.28293   0.411635   \n",
       "\n",
       "                                        PR-score   ROC AUC  \n",
       "Positive class fraction = 0.11          0.356719  0.784093  \n",
       "Positive class fraction = 0.21          0.533307  0.797007  \n",
       "Positive class fraction = 0.33          0.678596  0.806856  \n",
       "Positive class fraction = 0.42          0.754367  0.812276  \n",
       "Positive class fraction = 0.47          0.796059  0.817358  \n",
       "Positive class fraction = 0.52          0.827556  0.822084  \n",
       "RandomUnderSampler                      0.810379  0.817704  \n",
       "NearMiss UnderSampler                   0.921433  0.906948  \n",
       "InstanceHardnessThreshold UnderSampler  0.588451  0.853209  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Теперь перейдем к работе с признаками. Ранее вы реализовали несколько стратегий для обработки пропущенных значений. Сравните эти стратегии между собой с помощью оценки качества моделей кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка пропущенных значений сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?\n",
    "\n",
    "* Качество классификации не сильно меняется при измении способа обработки пропущенных значений, иногда - незначительно ухудшает;\n",
    "* Наиболее оптимальным с точки зрения качества является замена пропущенных значений матожиданием и нормализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x212 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8205377 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformed['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_transformed_median = transform_data(data=data, imput_method='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_transformed_freq = transform_data(data=data, imput_method='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_transformed_mean = transform_data(data=data, imput_method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_frac = 0.075\n",
    "\n",
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed['X'].tocsr(), y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' fill NaNs with means + normalization'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_frac = 0.075\n",
    "\n",
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed_mean['X'].tocsr(), y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' fill NaNs with means, no normalization'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed_median['X'].tocsr(), y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' fill NaNs with medians'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed_freq['X'].tocsr(), y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' fill NaNs with the most frequent'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision test</th>\n",
       "      <th>precision train</th>\n",
       "      <th>recall test</th>\n",
       "      <th>recall train</th>\n",
       "      <th>f1-score test</th>\n",
       "      <th>f1-score train</th>\n",
       "      <th>PR-score test</th>\n",
       "      <th>PR-score train</th>\n",
       "      <th>ROC AUC test</th>\n",
       "      <th>ROC AUC train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.543336</td>\n",
       "      <td>0.802362</td>\n",
       "      <td>0.0302397</td>\n",
       "      <td>0.048051</td>\n",
       "      <td>0.0572347</td>\n",
       "      <td>0.0905694</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>0.37886</td>\n",
       "      <td>0.736245</td>\n",
       "      <td>0.7939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.580892</td>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.277421</td>\n",
       "      <td>0.432115</td>\n",
       "      <td>0.555641</td>\n",
       "      <td>0.734751</td>\n",
       "      <td>0.808143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.735712</td>\n",
       "      <td>0.346785</td>\n",
       "      <td>0.410786</td>\n",
       "      <td>0.45063</td>\n",
       "      <td>0.52707</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.693886</td>\n",
       "      <td>0.738533</td>\n",
       "      <td>0.81657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.651932</td>\n",
       "      <td>0.738259</td>\n",
       "      <td>0.52521</td>\n",
       "      <td>0.60988</td>\n",
       "      <td>0.581503</td>\n",
       "      <td>0.667917</td>\n",
       "      <td>0.658956</td>\n",
       "      <td>0.770807</td>\n",
       "      <td>0.735974</td>\n",
       "      <td>0.82293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.664977</td>\n",
       "      <td>0.740377</td>\n",
       "      <td>0.639449</td>\n",
       "      <td>0.711441</td>\n",
       "      <td>0.651841</td>\n",
       "      <td>0.725551</td>\n",
       "      <td>0.705328</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.736613</td>\n",
       "      <td>0.830574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.670118</td>\n",
       "      <td>0.74205</td>\n",
       "      <td>0.718073</td>\n",
       "      <td>0.793011</td>\n",
       "      <td>0.693176</td>\n",
       "      <td>0.766639</td>\n",
       "      <td>0.733713</td>\n",
       "      <td>0.839188</td>\n",
       "      <td>0.732077</td>\n",
       "      <td>0.835514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.667147</td>\n",
       "      <td>0.734895</td>\n",
       "      <td>0.694553</td>\n",
       "      <td>0.767053</td>\n",
       "      <td>0.68044</td>\n",
       "      <td>0.750612</td>\n",
       "      <td>0.724784</td>\n",
       "      <td>0.823841</td>\n",
       "      <td>0.735272</td>\n",
       "      <td>0.830512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.824846</td>\n",
       "      <td>0.897813</td>\n",
       "      <td>0.67541</td>\n",
       "      <td>0.743784</td>\n",
       "      <td>0.742652</td>\n",
       "      <td>0.813564</td>\n",
       "      <td>0.877452</td>\n",
       "      <td>0.929137</td>\n",
       "      <td>0.852617</td>\n",
       "      <td>0.915934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.686542</td>\n",
       "      <td>0.767378</td>\n",
       "      <td>0.257055</td>\n",
       "      <td>0.291751</td>\n",
       "      <td>0.373709</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.512349</td>\n",
       "      <td>0.598667</td>\n",
       "      <td>0.816627</td>\n",
       "      <td>0.858223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means + normalization</th>\n",
       "      <td>0.670491</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717065</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692881</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733191</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.731737</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means, no normalization</th>\n",
       "      <td>0.669585</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717064</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692417</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733693</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.73207</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with medians</th>\n",
       "      <td>0.653652</td>\n",
       "      <td>0.726114</td>\n",
       "      <td>0.725471</td>\n",
       "      <td>0.80788</td>\n",
       "      <td>0.687651</td>\n",
       "      <td>0.764801</td>\n",
       "      <td>0.724906</td>\n",
       "      <td>0.83455</td>\n",
       "      <td>0.723056</td>\n",
       "      <td>0.83121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with the most frequent</th>\n",
       "      <td>0.654692</td>\n",
       "      <td>0.729067</td>\n",
       "      <td>0.726476</td>\n",
       "      <td>0.804772</td>\n",
       "      <td>0.688676</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.726293</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.724057</td>\n",
       "      <td>0.832352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision test  \\\n",
       "Positive class fraction = 0.11                           0.543336   \n",
       "Positive class fraction = 0.21                           0.580892   \n",
       "Positive class fraction = 0.33                           0.645367   \n",
       "Positive class fraction = 0.42                           0.651932   \n",
       "Positive class fraction = 0.47                           0.664977   \n",
       "Positive class fraction = 0.52                           0.670118   \n",
       "RandomUnderSampler                                       0.667147   \n",
       "NearMiss UnderSampler                                    0.824846   \n",
       "InstanceHardnessThreshold UnderSampler                   0.686542   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.670491   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.669585   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.653652   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.654692   \n",
       "\n",
       "                                                   precision train  \\\n",
       "Positive class fraction = 0.11                            0.802362   \n",
       "Positive class fraction = 0.21                              0.7348   \n",
       "Positive class fraction = 0.33                            0.735712   \n",
       "Positive class fraction = 0.42                            0.738259   \n",
       "Positive class fraction = 0.47                            0.740377   \n",
       "Positive class fraction = 0.52                             0.74205   \n",
       "RandomUnderSampler                                        0.734895   \n",
       "NearMiss UnderSampler                                     0.897813   \n",
       "InstanceHardnessThreshold UnderSampler                    0.767378   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.741787   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.741787   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.726114   \n",
       "Positive class fraction = 0.52 fill NaNs with t...        0.729067   \n",
       "\n",
       "                                                   recall test recall train  \\\n",
       "Positive class fraction = 0.11                       0.0302397     0.048051   \n",
       "Positive class fraction = 0.21                         0.13273     0.171034   \n",
       "Positive class fraction = 0.33                        0.346785     0.410786   \n",
       "Positive class fraction = 0.42                         0.52521      0.60988   \n",
       "Positive class fraction = 0.47                        0.639449     0.711441   \n",
       "Positive class fraction = 0.52                        0.718073     0.793011   \n",
       "RandomUnderSampler                                    0.694553     0.767053   \n",
       "NearMiss UnderSampler                                  0.67541     0.743784   \n",
       "InstanceHardnessThreshold UnderSampler                0.257055     0.291751   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.717065     0.793095   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.717064     0.793095   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.725471      0.80788   \n",
       "Positive class fraction = 0.52 fill NaNs with t...    0.726476     0.804772   \n",
       "\n",
       "                                                   f1-score test  \\\n",
       "Positive class fraction = 0.11                         0.0572347   \n",
       "Positive class fraction = 0.21                          0.215669   \n",
       "Positive class fraction = 0.33                           0.45063   \n",
       "Positive class fraction = 0.42                          0.581503   \n",
       "Positive class fraction = 0.47                          0.651841   \n",
       "Positive class fraction = 0.52                          0.693176   \n",
       "RandomUnderSampler                                       0.68044   \n",
       "NearMiss UnderSampler                                   0.742652   \n",
       "InstanceHardnessThreshold UnderSampler                  0.373709   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.692881   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.692417   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.687651   \n",
       "Positive class fraction = 0.52 fill NaNs with t...      0.688676   \n",
       "\n",
       "                                                   f1-score train  \\\n",
       "Positive class fraction = 0.11                          0.0905694   \n",
       "Positive class fraction = 0.21                           0.277421   \n",
       "Positive class fraction = 0.33                            0.52707   \n",
       "Positive class fraction = 0.42                           0.667917   \n",
       "Positive class fraction = 0.47                           0.725551   \n",
       "Positive class fraction = 0.52                           0.766639   \n",
       "RandomUnderSampler                                       0.750612   \n",
       "NearMiss UnderSampler                                    0.813564   \n",
       "InstanceHardnessThreshold UnderSampler                   0.422743   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.766533   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.766533   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.764801   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.765019   \n",
       "\n",
       "                                                   PR-score test  \\\n",
       "Positive class fraction = 0.11                          0.267859   \n",
       "Positive class fraction = 0.21                          0.432115   \n",
       "Positive class fraction = 0.33                          0.580529   \n",
       "Positive class fraction = 0.42                          0.658956   \n",
       "Positive class fraction = 0.47                          0.705328   \n",
       "Positive class fraction = 0.52                          0.733713   \n",
       "RandomUnderSampler                                      0.724784   \n",
       "NearMiss UnderSampler                                   0.877452   \n",
       "InstanceHardnessThreshold UnderSampler                  0.512349   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.733191   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.733693   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.724906   \n",
       "Positive class fraction = 0.52 fill NaNs with t...      0.726293   \n",
       "\n",
       "                                                   PR-score train  \\\n",
       "Positive class fraction = 0.11                            0.37886   \n",
       "Positive class fraction = 0.21                           0.555641   \n",
       "Positive class fraction = 0.33                           0.693886   \n",
       "Positive class fraction = 0.42                           0.770807   \n",
       "Positive class fraction = 0.47                           0.811321   \n",
       "Positive class fraction = 0.52                           0.839188   \n",
       "RandomUnderSampler                                       0.823841   \n",
       "NearMiss UnderSampler                                    0.929137   \n",
       "InstanceHardnessThreshold UnderSampler                   0.598667   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.839055   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.839055   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.83455   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.835821   \n",
       "\n",
       "                                                   ROC AUC test ROC AUC train  \n",
       "Positive class fraction = 0.11                         0.736245        0.7939  \n",
       "Positive class fraction = 0.21                         0.734751      0.808143  \n",
       "Positive class fraction = 0.33                         0.738533       0.81657  \n",
       "Positive class fraction = 0.42                         0.735974       0.82293  \n",
       "Positive class fraction = 0.47                         0.736613      0.830574  \n",
       "Positive class fraction = 0.52                         0.732077      0.835514  \n",
       "RandomUnderSampler                                     0.735272      0.830512  \n",
       "NearMiss UnderSampler                                  0.852617      0.915934  \n",
       "InstanceHardnessThreshold UnderSampler                 0.816627      0.858223  \n",
       "Positive class fraction = 0.52 fill NaNs with m...     0.731737       0.83515  \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.73207       0.83515  \n",
       "Positive class fraction = 0.52 fill NaNs with m...     0.723056       0.83121  \n",
       "Positive class fraction = 0.52 fill NaNs with t...     0.724057      0.832352  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>PR-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.766467</td>\n",
       "      <td>0.0430108</td>\n",
       "      <td>0.0814508</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.784093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.70901</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>0.262798</td>\n",
       "      <td>0.533307</td>\n",
       "      <td>0.797007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.721351</td>\n",
       "      <td>0.401882</td>\n",
       "      <td>0.516185</td>\n",
       "      <td>0.678596</td>\n",
       "      <td>0.806856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.72439</td>\n",
       "      <td>0.59879</td>\n",
       "      <td>0.655629</td>\n",
       "      <td>0.754367</td>\n",
       "      <td>0.812276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.728933</td>\n",
       "      <td>0.697581</td>\n",
       "      <td>0.712912</td>\n",
       "      <td>0.796059</td>\n",
       "      <td>0.817358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.721953</td>\n",
       "      <td>0.750336</td>\n",
       "      <td>0.735871</td>\n",
       "      <td>0.810379</td>\n",
       "      <td>0.817704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.884412</td>\n",
       "      <td>0.730175</td>\n",
       "      <td>0.799926</td>\n",
       "      <td>0.921433</td>\n",
       "      <td>0.906948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.755157</td>\n",
       "      <td>0.28293</td>\n",
       "      <td>0.411635</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.853209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means + normalization</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means, no normalization</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with medians</th>\n",
       "      <td>0.716192</td>\n",
       "      <td>0.793683</td>\n",
       "      <td>0.752949</td>\n",
       "      <td>0.823079</td>\n",
       "      <td>0.817187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with the most frequent</th>\n",
       "      <td>0.716924</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.75426</td>\n",
       "      <td>0.821945</td>\n",
       "      <td>0.817341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision     recall  \\\n",
       "Positive class fraction = 0.11                      0.766467  0.0430108   \n",
       "Positive class fraction = 0.21                       0.70901    0.16129   \n",
       "Positive class fraction = 0.33                      0.721351   0.401882   \n",
       "Positive class fraction = 0.42                       0.72439    0.59879   \n",
       "Positive class fraction = 0.47                      0.728933   0.697581   \n",
       "Positive class fraction = 0.52                      0.731923   0.778898   \n",
       "RandomUnderSampler                                  0.721953   0.750336   \n",
       "NearMiss UnderSampler                               0.884412   0.730175   \n",
       "InstanceHardnessThreshold UnderSampler              0.755157    0.28293   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.731923   0.778898   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.731923   0.778898   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.716192   0.793683   \n",
       "Positive class fraction = 0.52 fill NaNs with t...  0.716924   0.795699   \n",
       "\n",
       "                                                     f1-score  PR-score  \\\n",
       "Positive class fraction = 0.11                      0.0814508  0.356719   \n",
       "Positive class fraction = 0.21                       0.262798  0.533307   \n",
       "Positive class fraction = 0.33                       0.516185  0.678596   \n",
       "Positive class fraction = 0.42                       0.655629  0.754367   \n",
       "Positive class fraction = 0.47                       0.712912  0.796059   \n",
       "Positive class fraction = 0.52                        0.75468  0.827556   \n",
       "RandomUnderSampler                                   0.735871  0.810379   \n",
       "NearMiss UnderSampler                                0.799926  0.921433   \n",
       "InstanceHardnessThreshold UnderSampler               0.411635  0.588451   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.75468  0.827556   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.75468  0.827556   \n",
       "Positive class fraction = 0.52 fill NaNs with m...   0.752949  0.823079   \n",
       "Positive class fraction = 0.52 fill NaNs with t...    0.75426  0.821945   \n",
       "\n",
       "                                                     ROC AUC  \n",
       "Positive class fraction = 0.11                      0.784093  \n",
       "Positive class fraction = 0.21                      0.797007  \n",
       "Positive class fraction = 0.33                      0.806856  \n",
       "Positive class fraction = 0.42                      0.812276  \n",
       "Positive class fraction = 0.47                      0.817358  \n",
       "Positive class fraction = 0.52                      0.822084  \n",
       "RandomUnderSampler                                  0.817704  \n",
       "NearMiss UnderSampler                               0.906948  \n",
       "InstanceHardnessThreshold UnderSampler              0.853209  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.822084  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.822084  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.817187  \n",
       "Positive class fraction = 0.52 fill NaNs with t...  0.817341  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Также вы уже реализовали несколько стратегий для обработки категориальных признаков. Сравните эти стратегии между собой с помощью оценки качества моделей по кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка категориальных признаков сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?\n",
    " * Обработка только LableEncoder() и обработка композицией LableEncoder() + OneHotEncoder() дают одинаковое качество модели;\n",
    " * Оптимальным с точки зрения качества является обработка LableEncoder() без OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_transformed_LE = transform_data(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Applying a OneHotEncoder to categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_transformed_LE_OHE = transform_data(data=data, is_OHE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed_LE['X'].tocsr(), y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' + LableEncoder()'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed_LE_OHE['X'].tocsr(), y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' + LableEncoder() + OneHotEncoder()'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision test</th>\n",
       "      <th>precision train</th>\n",
       "      <th>recall test</th>\n",
       "      <th>recall train</th>\n",
       "      <th>f1-score test</th>\n",
       "      <th>f1-score train</th>\n",
       "      <th>PR-score test</th>\n",
       "      <th>PR-score train</th>\n",
       "      <th>ROC AUC test</th>\n",
       "      <th>ROC AUC train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.543336</td>\n",
       "      <td>0.802362</td>\n",
       "      <td>0.0302397</td>\n",
       "      <td>0.048051</td>\n",
       "      <td>0.0572347</td>\n",
       "      <td>0.0905694</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>0.37886</td>\n",
       "      <td>0.736245</td>\n",
       "      <td>0.7939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.580892</td>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.277421</td>\n",
       "      <td>0.432115</td>\n",
       "      <td>0.555641</td>\n",
       "      <td>0.734751</td>\n",
       "      <td>0.808143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.735712</td>\n",
       "      <td>0.346785</td>\n",
       "      <td>0.410786</td>\n",
       "      <td>0.45063</td>\n",
       "      <td>0.52707</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.693886</td>\n",
       "      <td>0.738533</td>\n",
       "      <td>0.81657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.651932</td>\n",
       "      <td>0.738259</td>\n",
       "      <td>0.52521</td>\n",
       "      <td>0.60988</td>\n",
       "      <td>0.581503</td>\n",
       "      <td>0.667917</td>\n",
       "      <td>0.658956</td>\n",
       "      <td>0.770807</td>\n",
       "      <td>0.735974</td>\n",
       "      <td>0.82293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.664977</td>\n",
       "      <td>0.740377</td>\n",
       "      <td>0.639449</td>\n",
       "      <td>0.711441</td>\n",
       "      <td>0.651841</td>\n",
       "      <td>0.725551</td>\n",
       "      <td>0.705328</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.736613</td>\n",
       "      <td>0.830574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.670118</td>\n",
       "      <td>0.74205</td>\n",
       "      <td>0.718073</td>\n",
       "      <td>0.793011</td>\n",
       "      <td>0.693176</td>\n",
       "      <td>0.766639</td>\n",
       "      <td>0.733713</td>\n",
       "      <td>0.839188</td>\n",
       "      <td>0.732077</td>\n",
       "      <td>0.835514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.667147</td>\n",
       "      <td>0.734895</td>\n",
       "      <td>0.694553</td>\n",
       "      <td>0.767053</td>\n",
       "      <td>0.68044</td>\n",
       "      <td>0.750612</td>\n",
       "      <td>0.724784</td>\n",
       "      <td>0.823841</td>\n",
       "      <td>0.735272</td>\n",
       "      <td>0.830512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.824846</td>\n",
       "      <td>0.897813</td>\n",
       "      <td>0.67541</td>\n",
       "      <td>0.743784</td>\n",
       "      <td>0.742652</td>\n",
       "      <td>0.813564</td>\n",
       "      <td>0.877452</td>\n",
       "      <td>0.929137</td>\n",
       "      <td>0.852617</td>\n",
       "      <td>0.915934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.686542</td>\n",
       "      <td>0.767378</td>\n",
       "      <td>0.257055</td>\n",
       "      <td>0.291751</td>\n",
       "      <td>0.373709</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.512349</td>\n",
       "      <td>0.598667</td>\n",
       "      <td>0.816627</td>\n",
       "      <td>0.858223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means + normalization</th>\n",
       "      <td>0.670491</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717065</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692881</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733191</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.731737</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means, no normalization</th>\n",
       "      <td>0.669585</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717064</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692417</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733693</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.73207</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with medians</th>\n",
       "      <td>0.653652</td>\n",
       "      <td>0.726114</td>\n",
       "      <td>0.725471</td>\n",
       "      <td>0.80788</td>\n",
       "      <td>0.687651</td>\n",
       "      <td>0.764801</td>\n",
       "      <td>0.724906</td>\n",
       "      <td>0.83455</td>\n",
       "      <td>0.723056</td>\n",
       "      <td>0.83121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with the most frequent</th>\n",
       "      <td>0.654692</td>\n",
       "      <td>0.729067</td>\n",
       "      <td>0.726476</td>\n",
       "      <td>0.804772</td>\n",
       "      <td>0.688676</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.726293</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.724057</td>\n",
       "      <td>0.832352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder()</th>\n",
       "      <td>0.670007</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717065</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692641</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733419</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.731933</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder() + OneHotEncoder()</th>\n",
       "      <td>0.674134</td>\n",
       "      <td>0.733599</td>\n",
       "      <td>0.718411</td>\n",
       "      <td>0.78419</td>\n",
       "      <td>0.695426</td>\n",
       "      <td>0.758025</td>\n",
       "      <td>0.739827</td>\n",
       "      <td>0.829013</td>\n",
       "      <td>0.73708</td>\n",
       "      <td>0.82366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision test  \\\n",
       "Positive class fraction = 0.11                           0.543336   \n",
       "Positive class fraction = 0.21                           0.580892   \n",
       "Positive class fraction = 0.33                           0.645367   \n",
       "Positive class fraction = 0.42                           0.651932   \n",
       "Positive class fraction = 0.47                           0.664977   \n",
       "Positive class fraction = 0.52                           0.670118   \n",
       "RandomUnderSampler                                       0.667147   \n",
       "NearMiss UnderSampler                                    0.824846   \n",
       "InstanceHardnessThreshold UnderSampler                   0.686542   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.670491   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.669585   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.653652   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.654692   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.670007   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.674134   \n",
       "\n",
       "                                                   precision train  \\\n",
       "Positive class fraction = 0.11                            0.802362   \n",
       "Positive class fraction = 0.21                              0.7348   \n",
       "Positive class fraction = 0.33                            0.735712   \n",
       "Positive class fraction = 0.42                            0.738259   \n",
       "Positive class fraction = 0.47                            0.740377   \n",
       "Positive class fraction = 0.52                             0.74205   \n",
       "RandomUnderSampler                                        0.734895   \n",
       "NearMiss UnderSampler                                     0.897813   \n",
       "InstanceHardnessThreshold UnderSampler                    0.767378   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.741787   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.741787   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.726114   \n",
       "Positive class fraction = 0.52 fill NaNs with t...        0.729067   \n",
       "Positive class fraction = 0.52 + LableEncoder()           0.741787   \n",
       "Positive class fraction = 0.52 + LableEncoder()...        0.733599   \n",
       "\n",
       "                                                   recall test recall train  \\\n",
       "Positive class fraction = 0.11                       0.0302397     0.048051   \n",
       "Positive class fraction = 0.21                         0.13273     0.171034   \n",
       "Positive class fraction = 0.33                        0.346785     0.410786   \n",
       "Positive class fraction = 0.42                         0.52521      0.60988   \n",
       "Positive class fraction = 0.47                        0.639449     0.711441   \n",
       "Positive class fraction = 0.52                        0.718073     0.793011   \n",
       "RandomUnderSampler                                    0.694553     0.767053   \n",
       "NearMiss UnderSampler                                  0.67541     0.743784   \n",
       "InstanceHardnessThreshold UnderSampler                0.257055     0.291751   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.717065     0.793095   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.717064     0.793095   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.725471      0.80788   \n",
       "Positive class fraction = 0.52 fill NaNs with t...    0.726476     0.804772   \n",
       "Positive class fraction = 0.52 + LableEncoder()       0.717065     0.793095   \n",
       "Positive class fraction = 0.52 + LableEncoder()...    0.718411      0.78419   \n",
       "\n",
       "                                                   f1-score test  \\\n",
       "Positive class fraction = 0.11                         0.0572347   \n",
       "Positive class fraction = 0.21                          0.215669   \n",
       "Positive class fraction = 0.33                           0.45063   \n",
       "Positive class fraction = 0.42                          0.581503   \n",
       "Positive class fraction = 0.47                          0.651841   \n",
       "Positive class fraction = 0.52                          0.693176   \n",
       "RandomUnderSampler                                       0.68044   \n",
       "NearMiss UnderSampler                                   0.742652   \n",
       "InstanceHardnessThreshold UnderSampler                  0.373709   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.692881   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.692417   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.687651   \n",
       "Positive class fraction = 0.52 fill NaNs with t...      0.688676   \n",
       "Positive class fraction = 0.52 + LableEncoder()         0.692641   \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.695426   \n",
       "\n",
       "                                                   f1-score train  \\\n",
       "Positive class fraction = 0.11                          0.0905694   \n",
       "Positive class fraction = 0.21                           0.277421   \n",
       "Positive class fraction = 0.33                            0.52707   \n",
       "Positive class fraction = 0.42                           0.667917   \n",
       "Positive class fraction = 0.47                           0.725551   \n",
       "Positive class fraction = 0.52                           0.766639   \n",
       "RandomUnderSampler                                       0.750612   \n",
       "NearMiss UnderSampler                                    0.813564   \n",
       "InstanceHardnessThreshold UnderSampler                   0.422743   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.766533   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.766533   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.764801   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.765019   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.766533   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.758025   \n",
       "\n",
       "                                                   PR-score test  \\\n",
       "Positive class fraction = 0.11                          0.267859   \n",
       "Positive class fraction = 0.21                          0.432115   \n",
       "Positive class fraction = 0.33                          0.580529   \n",
       "Positive class fraction = 0.42                          0.658956   \n",
       "Positive class fraction = 0.47                          0.705328   \n",
       "Positive class fraction = 0.52                          0.733713   \n",
       "RandomUnderSampler                                      0.724784   \n",
       "NearMiss UnderSampler                                   0.877452   \n",
       "InstanceHardnessThreshold UnderSampler                  0.512349   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.733191   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.733693   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.724906   \n",
       "Positive class fraction = 0.52 fill NaNs with t...      0.726293   \n",
       "Positive class fraction = 0.52 + LableEncoder()         0.733419   \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.739827   \n",
       "\n",
       "                                                   PR-score train  \\\n",
       "Positive class fraction = 0.11                            0.37886   \n",
       "Positive class fraction = 0.21                           0.555641   \n",
       "Positive class fraction = 0.33                           0.693886   \n",
       "Positive class fraction = 0.42                           0.770807   \n",
       "Positive class fraction = 0.47                           0.811321   \n",
       "Positive class fraction = 0.52                           0.839188   \n",
       "RandomUnderSampler                                       0.823841   \n",
       "NearMiss UnderSampler                                    0.929137   \n",
       "InstanceHardnessThreshold UnderSampler                   0.598667   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.839055   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.839055   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.83455   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.835821   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.839055   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.829013   \n",
       "\n",
       "                                                   ROC AUC test ROC AUC train  \n",
       "Positive class fraction = 0.11                         0.736245        0.7939  \n",
       "Positive class fraction = 0.21                         0.734751      0.808143  \n",
       "Positive class fraction = 0.33                         0.738533       0.81657  \n",
       "Positive class fraction = 0.42                         0.735974       0.82293  \n",
       "Positive class fraction = 0.47                         0.736613      0.830574  \n",
       "Positive class fraction = 0.52                         0.732077      0.835514  \n",
       "RandomUnderSampler                                     0.735272      0.830512  \n",
       "NearMiss UnderSampler                                  0.852617      0.915934  \n",
       "InstanceHardnessThreshold UnderSampler                 0.816627      0.858223  \n",
       "Positive class fraction = 0.52 fill NaNs with m...     0.731737       0.83515  \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.73207       0.83515  \n",
       "Positive class fraction = 0.52 fill NaNs with m...     0.723056       0.83121  \n",
       "Positive class fraction = 0.52 fill NaNs with t...     0.724057      0.832352  \n",
       "Positive class fraction = 0.52 + LableEncoder()        0.731933       0.83515  \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.73708       0.82366  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>PR-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.766467</td>\n",
       "      <td>0.0430108</td>\n",
       "      <td>0.0814508</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.784093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.70901</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>0.262798</td>\n",
       "      <td>0.533307</td>\n",
       "      <td>0.797007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.721351</td>\n",
       "      <td>0.401882</td>\n",
       "      <td>0.516185</td>\n",
       "      <td>0.678596</td>\n",
       "      <td>0.806856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.72439</td>\n",
       "      <td>0.59879</td>\n",
       "      <td>0.655629</td>\n",
       "      <td>0.754367</td>\n",
       "      <td>0.812276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.728933</td>\n",
       "      <td>0.697581</td>\n",
       "      <td>0.712912</td>\n",
       "      <td>0.796059</td>\n",
       "      <td>0.817358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.721953</td>\n",
       "      <td>0.750336</td>\n",
       "      <td>0.735871</td>\n",
       "      <td>0.810379</td>\n",
       "      <td>0.817704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.884412</td>\n",
       "      <td>0.730175</td>\n",
       "      <td>0.799926</td>\n",
       "      <td>0.921433</td>\n",
       "      <td>0.906948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.755157</td>\n",
       "      <td>0.28293</td>\n",
       "      <td>0.411635</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.853209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means + normalization</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means, no normalization</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with medians</th>\n",
       "      <td>0.716192</td>\n",
       "      <td>0.793683</td>\n",
       "      <td>0.752949</td>\n",
       "      <td>0.823079</td>\n",
       "      <td>0.817187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with the most frequent</th>\n",
       "      <td>0.716924</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.75426</td>\n",
       "      <td>0.821945</td>\n",
       "      <td>0.817341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder()</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder() + OneHotEncoder()</th>\n",
       "      <td>0.731396</td>\n",
       "      <td>0.769489</td>\n",
       "      <td>0.749959</td>\n",
       "      <td>0.814671</td>\n",
       "      <td>0.811156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision     recall  \\\n",
       "Positive class fraction = 0.11                      0.766467  0.0430108   \n",
       "Positive class fraction = 0.21                       0.70901    0.16129   \n",
       "Positive class fraction = 0.33                      0.721351   0.401882   \n",
       "Positive class fraction = 0.42                       0.72439    0.59879   \n",
       "Positive class fraction = 0.47                      0.728933   0.697581   \n",
       "Positive class fraction = 0.52                      0.731923   0.778898   \n",
       "RandomUnderSampler                                  0.721953   0.750336   \n",
       "NearMiss UnderSampler                               0.884412   0.730175   \n",
       "InstanceHardnessThreshold UnderSampler              0.755157    0.28293   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.731923   0.778898   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.731923   0.778898   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.716192   0.793683   \n",
       "Positive class fraction = 0.52 fill NaNs with t...  0.716924   0.795699   \n",
       "Positive class fraction = 0.52 + LableEncoder()     0.731923   0.778898   \n",
       "Positive class fraction = 0.52 + LableEncoder()...  0.731396   0.769489   \n",
       "\n",
       "                                                     f1-score  PR-score  \\\n",
       "Positive class fraction = 0.11                      0.0814508  0.356719   \n",
       "Positive class fraction = 0.21                       0.262798  0.533307   \n",
       "Positive class fraction = 0.33                       0.516185  0.678596   \n",
       "Positive class fraction = 0.42                       0.655629  0.754367   \n",
       "Positive class fraction = 0.47                       0.712912  0.796059   \n",
       "Positive class fraction = 0.52                        0.75468  0.827556   \n",
       "RandomUnderSampler                                   0.735871  0.810379   \n",
       "NearMiss UnderSampler                                0.799926  0.921433   \n",
       "InstanceHardnessThreshold UnderSampler               0.411635  0.588451   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.75468  0.827556   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.75468  0.827556   \n",
       "Positive class fraction = 0.52 fill NaNs with m...   0.752949  0.823079   \n",
       "Positive class fraction = 0.52 fill NaNs with t...    0.75426  0.821945   \n",
       "Positive class fraction = 0.52 + LableEncoder()       0.75468  0.827556   \n",
       "Positive class fraction = 0.52 + LableEncoder()...   0.749959  0.814671   \n",
       "\n",
       "                                                     ROC AUC  \n",
       "Positive class fraction = 0.11                      0.784093  \n",
       "Positive class fraction = 0.21                      0.797007  \n",
       "Positive class fraction = 0.33                      0.806856  \n",
       "Positive class fraction = 0.42                      0.812276  \n",
       "Positive class fraction = 0.47                      0.817358  \n",
       "Positive class fraction = 0.52                      0.822084  \n",
       "RandomUnderSampler                                  0.817704  \n",
       "NearMiss UnderSampler                               0.906948  \n",
       "InstanceHardnessThreshold UnderSampler              0.853209  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.822084  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.822084  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.817187  \n",
       "Positive class fraction = 0.52 fill NaNs with t...  0.817341  \n",
       "Positive class fraction = 0.52 + LableEncoder()     0.822084  \n",
       "Positive class fraction = 0.52 + LableEncoder()...  0.811156  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Все ли признаки оказались полезными для построения моделей? Проведите процедуру отбора признаков, попробуйте разные варианты отбора (обратите внимание на модуль `sklearn.feature_selection`). Например, можно выбрасывать случайные признаки или строить отбор на основе l1-регуляризации - отфильтровать из обучения признаки, которые получат нулевой вес при построении регрессии с l1-регуляризацией (`sklearn.linear_model.Lasso`). И всегда можно придумать что-то своё=) Попробуйте как минимум 2 различные стратегии, сравните результаты. Помог ли отбор признаков улучшить качество модели? Поясните свой ответ.\n",
    "\n",
    "* Отбор признаков как на базе l1-регуляризации, так и на базе дерьевьев, сохранил качество, но при этом уменьшил количество используемых признаков, то есть итоговая модель требует меньшее количество информации, меньше вычислительных мощностей. Это увеличивает быстродействие, поэтому качество модели в целом улучшилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Appling the l1-based feature selection method\n",
      "New shape of X-data is (40000, 86)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_transformed_l1 = transform_data(data=data, y=y, feature_selection='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Appling the tree-based feature selection method\n",
      "New shape of X-data is (40000, 70)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_transformed_tree = transform_data(data=data, y=y, feature_selection='tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_transformed_top_20_num = transform_data(data=data,y=y, num_cols=num_top_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Applying a OneHotEncoder to categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Appling the l1-based feature selection method\n",
      "New shape of X-data is (40000, 70)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_result_l1_OHE = transform_data(data=data, y=y, is_OHE=True, feature_selection='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Applying a OneHotEncoder to categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Appling the tree-based feature selection method\n",
      "New shape of X-data is (40000, 9504)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_result_tree_OHE = transform_data(data=data, y=y, is_OHE=True, feature_selection='tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_frac = 0.075\n",
    "\n",
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed_l1['X'], y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' + l1-based feature selection'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_frac = 0.075\n",
    "\n",
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed_tree['X'], y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' + tree-based feature selection'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_frac = 0.075\n",
    "\n",
    "X_1, y_1, pos_frac, _, _ = resample_data(data_transformed_top_20_num['X'], y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' + 20 selected correlation-based features'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_frac = 0.075\n",
    "\n",
    "X_1, y_1, pos_frac, _, _ = resample_data(data_result_l1_OHE['X'], y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + '+ OneHotEncoder() + l1-based feature selection'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size is 5752, the positive and negative class fractions are 0.52 and 0.48\n",
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "each_frac = 0.075\n",
    "\n",
    "X_1, y_1, pos_frac, _, _ = resample_data(data_result_tree_OHE['X'], y, 1., each_frac)\n",
    "configuration='Positive class fraction = ' + str(round(pos_frac, 2)) + ' + OneHotEncoder() + tree-based feature selection'\n",
    "\n",
    "get_scores(X_1, y_1, gbc, configuration, scores_cv)\n",
    "gbc.fit(X_1, y_1)\n",
    "get_scores_hold_out(X_1, y_1, gbc, configuration, scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision test</th>\n",
       "      <th>precision train</th>\n",
       "      <th>recall test</th>\n",
       "      <th>recall train</th>\n",
       "      <th>f1-score test</th>\n",
       "      <th>f1-score train</th>\n",
       "      <th>PR-score test</th>\n",
       "      <th>PR-score train</th>\n",
       "      <th>ROC AUC test</th>\n",
       "      <th>ROC AUC train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.543336</td>\n",
       "      <td>0.802362</td>\n",
       "      <td>0.0302397</td>\n",
       "      <td>0.048051</td>\n",
       "      <td>0.0572347</td>\n",
       "      <td>0.0905694</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>0.37886</td>\n",
       "      <td>0.736245</td>\n",
       "      <td>0.7939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.580892</td>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.277421</td>\n",
       "      <td>0.432115</td>\n",
       "      <td>0.555641</td>\n",
       "      <td>0.734751</td>\n",
       "      <td>0.808143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.735712</td>\n",
       "      <td>0.346785</td>\n",
       "      <td>0.410786</td>\n",
       "      <td>0.45063</td>\n",
       "      <td>0.52707</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.693886</td>\n",
       "      <td>0.738533</td>\n",
       "      <td>0.81657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.651932</td>\n",
       "      <td>0.738259</td>\n",
       "      <td>0.52521</td>\n",
       "      <td>0.60988</td>\n",
       "      <td>0.581503</td>\n",
       "      <td>0.667917</td>\n",
       "      <td>0.658956</td>\n",
       "      <td>0.770807</td>\n",
       "      <td>0.735974</td>\n",
       "      <td>0.82293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.664977</td>\n",
       "      <td>0.740377</td>\n",
       "      <td>0.639449</td>\n",
       "      <td>0.711441</td>\n",
       "      <td>0.651841</td>\n",
       "      <td>0.725551</td>\n",
       "      <td>0.705328</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.736613</td>\n",
       "      <td>0.830574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.670118</td>\n",
       "      <td>0.74205</td>\n",
       "      <td>0.718073</td>\n",
       "      <td>0.793011</td>\n",
       "      <td>0.693176</td>\n",
       "      <td>0.766639</td>\n",
       "      <td>0.733713</td>\n",
       "      <td>0.839188</td>\n",
       "      <td>0.732077</td>\n",
       "      <td>0.835514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.667147</td>\n",
       "      <td>0.734895</td>\n",
       "      <td>0.694553</td>\n",
       "      <td>0.767053</td>\n",
       "      <td>0.68044</td>\n",
       "      <td>0.750612</td>\n",
       "      <td>0.724784</td>\n",
       "      <td>0.823841</td>\n",
       "      <td>0.735272</td>\n",
       "      <td>0.830512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.824846</td>\n",
       "      <td>0.897813</td>\n",
       "      <td>0.67541</td>\n",
       "      <td>0.743784</td>\n",
       "      <td>0.742652</td>\n",
       "      <td>0.813564</td>\n",
       "      <td>0.877452</td>\n",
       "      <td>0.929137</td>\n",
       "      <td>0.852617</td>\n",
       "      <td>0.915934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.686542</td>\n",
       "      <td>0.767378</td>\n",
       "      <td>0.257055</td>\n",
       "      <td>0.291751</td>\n",
       "      <td>0.373709</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.512349</td>\n",
       "      <td>0.598667</td>\n",
       "      <td>0.816627</td>\n",
       "      <td>0.858223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means + normalization</th>\n",
       "      <td>0.670491</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717065</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692881</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733191</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.731737</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means, no normalization</th>\n",
       "      <td>0.669585</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717064</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692417</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733693</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.73207</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with medians</th>\n",
       "      <td>0.653652</td>\n",
       "      <td>0.726114</td>\n",
       "      <td>0.725471</td>\n",
       "      <td>0.80788</td>\n",
       "      <td>0.687651</td>\n",
       "      <td>0.764801</td>\n",
       "      <td>0.724906</td>\n",
       "      <td>0.83455</td>\n",
       "      <td>0.723056</td>\n",
       "      <td>0.83121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with the most frequent</th>\n",
       "      <td>0.654692</td>\n",
       "      <td>0.729067</td>\n",
       "      <td>0.726476</td>\n",
       "      <td>0.804772</td>\n",
       "      <td>0.688676</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.726293</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.724057</td>\n",
       "      <td>0.832352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder()</th>\n",
       "      <td>0.670007</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717065</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692641</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733419</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.731933</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder() + OneHotEncoder()</th>\n",
       "      <td>0.674134</td>\n",
       "      <td>0.733599</td>\n",
       "      <td>0.718411</td>\n",
       "      <td>0.78419</td>\n",
       "      <td>0.695426</td>\n",
       "      <td>0.758025</td>\n",
       "      <td>0.739827</td>\n",
       "      <td>0.829013</td>\n",
       "      <td>0.73708</td>\n",
       "      <td>0.82366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + l1-based feature selection</th>\n",
       "      <td>0.671252</td>\n",
       "      <td>0.730653</td>\n",
       "      <td>0.723117</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.696023</td>\n",
       "      <td>0.755079</td>\n",
       "      <td>0.737412</td>\n",
       "      <td>0.82327</td>\n",
       "      <td>0.735509</td>\n",
       "      <td>0.821125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + tree-based feature selection</th>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.746206</td>\n",
       "      <td>0.706656</td>\n",
       "      <td>0.794271</td>\n",
       "      <td>0.687401</td>\n",
       "      <td>0.769463</td>\n",
       "      <td>0.733625</td>\n",
       "      <td>0.845065</td>\n",
       "      <td>0.731626</td>\n",
       "      <td>0.839669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + 20 selected correlation-based features</th>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.73285</td>\n",
       "      <td>0.705307</td>\n",
       "      <td>0.775285</td>\n",
       "      <td>0.687385</td>\n",
       "      <td>0.753432</td>\n",
       "      <td>0.735469</td>\n",
       "      <td>0.823183</td>\n",
       "      <td>0.731638</td>\n",
       "      <td>0.819441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52+ OneHotEncoder() + l1-based feature selection</th>\n",
       "      <td>0.674474</td>\n",
       "      <td>0.725556</td>\n",
       "      <td>0.717736</td>\n",
       "      <td>0.773857</td>\n",
       "      <td>0.695202</td>\n",
       "      <td>0.748912</td>\n",
       "      <td>0.737743</td>\n",
       "      <td>0.810676</td>\n",
       "      <td>0.737681</td>\n",
       "      <td>0.811592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + OneHotEncoder() + tree-based feature selection</th>\n",
       "      <td>0.675147</td>\n",
       "      <td>0.73451</td>\n",
       "      <td>0.713706</td>\n",
       "      <td>0.786038</td>\n",
       "      <td>0.693682</td>\n",
       "      <td>0.759374</td>\n",
       "      <td>0.739823</td>\n",
       "      <td>0.828341</td>\n",
       "      <td>0.737301</td>\n",
       "      <td>0.823631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision test  \\\n",
       "Positive class fraction = 0.11                           0.543336   \n",
       "Positive class fraction = 0.21                           0.580892   \n",
       "Positive class fraction = 0.33                           0.645367   \n",
       "Positive class fraction = 0.42                           0.651932   \n",
       "Positive class fraction = 0.47                           0.664977   \n",
       "Positive class fraction = 0.52                           0.670118   \n",
       "RandomUnderSampler                                       0.667147   \n",
       "NearMiss UnderSampler                                    0.824846   \n",
       "InstanceHardnessThreshold UnderSampler                   0.686542   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.670491   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.669585   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.653652   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.654692   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.670007   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.674134   \n",
       "Positive class fraction = 0.52 + l1-based featu...       0.671252   \n",
       "Positive class fraction = 0.52 + tree-based fea...       0.669753   \n",
       "Positive class fraction = 0.52 + 20 selected co...       0.670752   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...       0.674474   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...       0.675147   \n",
       "\n",
       "                                                   precision train  \\\n",
       "Positive class fraction = 0.11                            0.802362   \n",
       "Positive class fraction = 0.21                              0.7348   \n",
       "Positive class fraction = 0.33                            0.735712   \n",
       "Positive class fraction = 0.42                            0.738259   \n",
       "Positive class fraction = 0.47                            0.740377   \n",
       "Positive class fraction = 0.52                             0.74205   \n",
       "RandomUnderSampler                                        0.734895   \n",
       "NearMiss UnderSampler                                     0.897813   \n",
       "InstanceHardnessThreshold UnderSampler                    0.767378   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.741787   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.741787   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.726114   \n",
       "Positive class fraction = 0.52 fill NaNs with t...        0.729067   \n",
       "Positive class fraction = 0.52 + LableEncoder()           0.741787   \n",
       "Positive class fraction = 0.52 + LableEncoder()...        0.733599   \n",
       "Positive class fraction = 0.52 + l1-based featu...        0.730653   \n",
       "Positive class fraction = 0.52 + tree-based fea...        0.746206   \n",
       "Positive class fraction = 0.52 + 20 selected co...         0.73285   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...        0.725556   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...         0.73451   \n",
       "\n",
       "                                                   recall test recall train  \\\n",
       "Positive class fraction = 0.11                       0.0302397     0.048051   \n",
       "Positive class fraction = 0.21                         0.13273     0.171034   \n",
       "Positive class fraction = 0.33                        0.346785     0.410786   \n",
       "Positive class fraction = 0.42                         0.52521      0.60988   \n",
       "Positive class fraction = 0.47                        0.639449     0.711441   \n",
       "Positive class fraction = 0.52                        0.718073     0.793011   \n",
       "RandomUnderSampler                                    0.694553     0.767053   \n",
       "NearMiss UnderSampler                                  0.67541     0.743784   \n",
       "InstanceHardnessThreshold UnderSampler                0.257055     0.291751   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.717065     0.793095   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.717064     0.793095   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.725471      0.80788   \n",
       "Positive class fraction = 0.52 fill NaNs with t...    0.726476     0.804772   \n",
       "Positive class fraction = 0.52 + LableEncoder()       0.717065     0.793095   \n",
       "Positive class fraction = 0.52 + LableEncoder()...    0.718411      0.78419   \n",
       "Positive class fraction = 0.52 + l1-based featu...    0.723117      0.78125   \n",
       "Positive class fraction = 0.52 + tree-based fea...    0.706656     0.794271   \n",
       "Positive class fraction = 0.52 + 20 selected co...    0.705307     0.775285   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...    0.717736     0.773857   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...    0.713706     0.786038   \n",
       "\n",
       "                                                   f1-score test  \\\n",
       "Positive class fraction = 0.11                         0.0572347   \n",
       "Positive class fraction = 0.21                          0.215669   \n",
       "Positive class fraction = 0.33                           0.45063   \n",
       "Positive class fraction = 0.42                          0.581503   \n",
       "Positive class fraction = 0.47                          0.651841   \n",
       "Positive class fraction = 0.52                          0.693176   \n",
       "RandomUnderSampler                                       0.68044   \n",
       "NearMiss UnderSampler                                   0.742652   \n",
       "InstanceHardnessThreshold UnderSampler                  0.373709   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.692881   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.692417   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.687651   \n",
       "Positive class fraction = 0.52 fill NaNs with t...      0.688676   \n",
       "Positive class fraction = 0.52 + LableEncoder()         0.692641   \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.695426   \n",
       "Positive class fraction = 0.52 + l1-based featu...      0.696023   \n",
       "Positive class fraction = 0.52 + tree-based fea...      0.687401   \n",
       "Positive class fraction = 0.52 + 20 selected co...      0.687385   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...      0.695202   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...      0.693682   \n",
       "\n",
       "                                                   f1-score train  \\\n",
       "Positive class fraction = 0.11                          0.0905694   \n",
       "Positive class fraction = 0.21                           0.277421   \n",
       "Positive class fraction = 0.33                            0.52707   \n",
       "Positive class fraction = 0.42                           0.667917   \n",
       "Positive class fraction = 0.47                           0.725551   \n",
       "Positive class fraction = 0.52                           0.766639   \n",
       "RandomUnderSampler                                       0.750612   \n",
       "NearMiss UnderSampler                                    0.813564   \n",
       "InstanceHardnessThreshold UnderSampler                   0.422743   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.766533   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.766533   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.764801   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.765019   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.766533   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.758025   \n",
       "Positive class fraction = 0.52 + l1-based featu...       0.755079   \n",
       "Positive class fraction = 0.52 + tree-based fea...       0.769463   \n",
       "Positive class fraction = 0.52 + 20 selected co...       0.753432   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...       0.748912   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...       0.759374   \n",
       "\n",
       "                                                   PR-score test  \\\n",
       "Positive class fraction = 0.11                          0.267859   \n",
       "Positive class fraction = 0.21                          0.432115   \n",
       "Positive class fraction = 0.33                          0.580529   \n",
       "Positive class fraction = 0.42                          0.658956   \n",
       "Positive class fraction = 0.47                          0.705328   \n",
       "Positive class fraction = 0.52                          0.733713   \n",
       "RandomUnderSampler                                      0.724784   \n",
       "NearMiss UnderSampler                                   0.877452   \n",
       "InstanceHardnessThreshold UnderSampler                  0.512349   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.733191   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.733693   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.724906   \n",
       "Positive class fraction = 0.52 fill NaNs with t...      0.726293   \n",
       "Positive class fraction = 0.52 + LableEncoder()         0.733419   \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.739827   \n",
       "Positive class fraction = 0.52 + l1-based featu...      0.737412   \n",
       "Positive class fraction = 0.52 + tree-based fea...      0.733625   \n",
       "Positive class fraction = 0.52 + 20 selected co...      0.735469   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...      0.737743   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...      0.739823   \n",
       "\n",
       "                                                   PR-score train  \\\n",
       "Positive class fraction = 0.11                            0.37886   \n",
       "Positive class fraction = 0.21                           0.555641   \n",
       "Positive class fraction = 0.33                           0.693886   \n",
       "Positive class fraction = 0.42                           0.770807   \n",
       "Positive class fraction = 0.47                           0.811321   \n",
       "Positive class fraction = 0.52                           0.839188   \n",
       "RandomUnderSampler                                       0.823841   \n",
       "NearMiss UnderSampler                                    0.929137   \n",
       "InstanceHardnessThreshold UnderSampler                   0.598667   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.839055   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.839055   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.83455   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.835821   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.839055   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.829013   \n",
       "Positive class fraction = 0.52 + l1-based featu...        0.82327   \n",
       "Positive class fraction = 0.52 + tree-based fea...       0.845065   \n",
       "Positive class fraction = 0.52 + 20 selected co...       0.823183   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...       0.810676   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...       0.828341   \n",
       "\n",
       "                                                   ROC AUC test ROC AUC train  \n",
       "Positive class fraction = 0.11                         0.736245        0.7939  \n",
       "Positive class fraction = 0.21                         0.734751      0.808143  \n",
       "Positive class fraction = 0.33                         0.738533       0.81657  \n",
       "Positive class fraction = 0.42                         0.735974       0.82293  \n",
       "Positive class fraction = 0.47                         0.736613      0.830574  \n",
       "Positive class fraction = 0.52                         0.732077      0.835514  \n",
       "RandomUnderSampler                                     0.735272      0.830512  \n",
       "NearMiss UnderSampler                                  0.852617      0.915934  \n",
       "InstanceHardnessThreshold UnderSampler                 0.816627      0.858223  \n",
       "Positive class fraction = 0.52 fill NaNs with m...     0.731737       0.83515  \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.73207       0.83515  \n",
       "Positive class fraction = 0.52 fill NaNs with m...     0.723056       0.83121  \n",
       "Positive class fraction = 0.52 fill NaNs with t...     0.724057      0.832352  \n",
       "Positive class fraction = 0.52 + LableEncoder()        0.731933       0.83515  \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.73708       0.82366  \n",
       "Positive class fraction = 0.52 + l1-based featu...     0.735509      0.821125  \n",
       "Positive class fraction = 0.52 + tree-based fea...     0.731626      0.839669  \n",
       "Positive class fraction = 0.52 + 20 selected co...     0.731638      0.819441  \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...     0.737681      0.811592  \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...     0.737301      0.823631  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>PR-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.766467</td>\n",
       "      <td>0.0430108</td>\n",
       "      <td>0.0814508</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.784093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.70901</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>0.262798</td>\n",
       "      <td>0.533307</td>\n",
       "      <td>0.797007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.721351</td>\n",
       "      <td>0.401882</td>\n",
       "      <td>0.516185</td>\n",
       "      <td>0.678596</td>\n",
       "      <td>0.806856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.72439</td>\n",
       "      <td>0.59879</td>\n",
       "      <td>0.655629</td>\n",
       "      <td>0.754367</td>\n",
       "      <td>0.812276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.728933</td>\n",
       "      <td>0.697581</td>\n",
       "      <td>0.712912</td>\n",
       "      <td>0.796059</td>\n",
       "      <td>0.817358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.721953</td>\n",
       "      <td>0.750336</td>\n",
       "      <td>0.735871</td>\n",
       "      <td>0.810379</td>\n",
       "      <td>0.817704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.884412</td>\n",
       "      <td>0.730175</td>\n",
       "      <td>0.799926</td>\n",
       "      <td>0.921433</td>\n",
       "      <td>0.906948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.755157</td>\n",
       "      <td>0.28293</td>\n",
       "      <td>0.411635</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.853209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means + normalization</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means, no normalization</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with medians</th>\n",
       "      <td>0.716192</td>\n",
       "      <td>0.793683</td>\n",
       "      <td>0.752949</td>\n",
       "      <td>0.823079</td>\n",
       "      <td>0.817187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with the most frequent</th>\n",
       "      <td>0.716924</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.75426</td>\n",
       "      <td>0.821945</td>\n",
       "      <td>0.817341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder()</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder() + OneHotEncoder()</th>\n",
       "      <td>0.731396</td>\n",
       "      <td>0.769489</td>\n",
       "      <td>0.749959</td>\n",
       "      <td>0.814671</td>\n",
       "      <td>0.811156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + l1-based feature selection</th>\n",
       "      <td>0.725167</td>\n",
       "      <td>0.767809</td>\n",
       "      <td>0.745879</td>\n",
       "      <td>0.812827</td>\n",
       "      <td>0.809617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + tree-based feature selection</th>\n",
       "      <td>0.73746</td>\n",
       "      <td>0.780578</td>\n",
       "      <td>0.758407</td>\n",
       "      <td>0.832506</td>\n",
       "      <td>0.825036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + 20 selected correlation-based features</th>\n",
       "      <td>0.727157</td>\n",
       "      <td>0.764785</td>\n",
       "      <td>0.745496</td>\n",
       "      <td>0.811537</td>\n",
       "      <td>0.806987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52+ OneHotEncoder() + l1-based feature selection</th>\n",
       "      <td>0.719539</td>\n",
       "      <td>0.756048</td>\n",
       "      <td>0.737342</td>\n",
       "      <td>0.799162</td>\n",
       "      <td>0.801018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + OneHotEncoder() + tree-based feature selection</th>\n",
       "      <td>0.729213</td>\n",
       "      <td>0.769153</td>\n",
       "      <td>0.748651</td>\n",
       "      <td>0.816683</td>\n",
       "      <td>0.813735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision     recall  \\\n",
       "Positive class fraction = 0.11                      0.766467  0.0430108   \n",
       "Positive class fraction = 0.21                       0.70901    0.16129   \n",
       "Positive class fraction = 0.33                      0.721351   0.401882   \n",
       "Positive class fraction = 0.42                       0.72439    0.59879   \n",
       "Positive class fraction = 0.47                      0.728933   0.697581   \n",
       "Positive class fraction = 0.52                      0.731923   0.778898   \n",
       "RandomUnderSampler                                  0.721953   0.750336   \n",
       "NearMiss UnderSampler                               0.884412   0.730175   \n",
       "InstanceHardnessThreshold UnderSampler              0.755157    0.28293   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.731923   0.778898   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.731923   0.778898   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.716192   0.793683   \n",
       "Positive class fraction = 0.52 fill NaNs with t...  0.716924   0.795699   \n",
       "Positive class fraction = 0.52 + LableEncoder()     0.731923   0.778898   \n",
       "Positive class fraction = 0.52 + LableEncoder()...  0.731396   0.769489   \n",
       "Positive class fraction = 0.52 + l1-based featu...  0.725167   0.767809   \n",
       "Positive class fraction = 0.52 + tree-based fea...   0.73746   0.780578   \n",
       "Positive class fraction = 0.52 + 20 selected co...  0.727157   0.764785   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...  0.719539   0.756048   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...  0.729213   0.769153   \n",
       "\n",
       "                                                     f1-score  PR-score  \\\n",
       "Positive class fraction = 0.11                      0.0814508  0.356719   \n",
       "Positive class fraction = 0.21                       0.262798  0.533307   \n",
       "Positive class fraction = 0.33                       0.516185  0.678596   \n",
       "Positive class fraction = 0.42                       0.655629  0.754367   \n",
       "Positive class fraction = 0.47                       0.712912  0.796059   \n",
       "Positive class fraction = 0.52                        0.75468  0.827556   \n",
       "RandomUnderSampler                                   0.735871  0.810379   \n",
       "NearMiss UnderSampler                                0.799926  0.921433   \n",
       "InstanceHardnessThreshold UnderSampler               0.411635  0.588451   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.75468  0.827556   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.75468  0.827556   \n",
       "Positive class fraction = 0.52 fill NaNs with m...   0.752949  0.823079   \n",
       "Positive class fraction = 0.52 fill NaNs with t...    0.75426  0.821945   \n",
       "Positive class fraction = 0.52 + LableEncoder()       0.75468  0.827556   \n",
       "Positive class fraction = 0.52 + LableEncoder()...   0.749959  0.814671   \n",
       "Positive class fraction = 0.52 + l1-based featu...   0.745879  0.812827   \n",
       "Positive class fraction = 0.52 + tree-based fea...   0.758407  0.832506   \n",
       "Positive class fraction = 0.52 + 20 selected co...   0.745496  0.811537   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...   0.737342  0.799162   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...   0.748651  0.816683   \n",
       "\n",
       "                                                     ROC AUC  \n",
       "Positive class fraction = 0.11                      0.784093  \n",
       "Positive class fraction = 0.21                      0.797007  \n",
       "Positive class fraction = 0.33                      0.806856  \n",
       "Positive class fraction = 0.42                      0.812276  \n",
       "Positive class fraction = 0.47                      0.817358  \n",
       "Positive class fraction = 0.52                      0.822084  \n",
       "RandomUnderSampler                                  0.817704  \n",
       "NearMiss UnderSampler                               0.906948  \n",
       "InstanceHardnessThreshold UnderSampler              0.853209  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.822084  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.822084  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.817187  \n",
       "Positive class fraction = 0.52 fill NaNs with t...  0.817341  \n",
       "Positive class fraction = 0.52 + LableEncoder()     0.822084  \n",
       "Positive class fraction = 0.52 + LableEncoder()...  0.811156  \n",
       "Positive class fraction = 0.52 + l1-based featu...  0.809617  \n",
       "Positive class fraction = 0.52 + tree-based fea...  0.825036  \n",
       "Positive class fraction = 0.52 + 20 selected co...  0.806987  \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...  0.801018  \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...  0.813735  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Подберите оптимальные параметры модели. Обратите внимание, что в зависимости от того, как вы обработали исходные данные, сделали ли балансировку классов, сколько объектов оставили в обучающей выборке и др. оптимальные значения параметров могут меняться. Возьмите наилучшее из ваших решений на текущий момент и проведите процедуру подбора параметров модели (обратите внимание на `sklearn.model_selection.GridSearchCV`) Как подбор параметров повлиял на качество модели?\n",
    "\n",
    "   * Качество модели незначительно улучшилось по сравнению с вариантом NearMiss UnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "near_miss = NearMiss(random_state=0)\n",
    "X_resampled_LE, y_resampled_LE = near_miss.fit_sample(data_transformed_tree['X'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5952, 70)\n"
     ]
    }
   ],
   "source": [
    "print(X_resampled_LE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = ['precision', 'recall', 'average_precision', 'roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(verbose=1)\n",
    "parameters = {'learning_rate':[0.1, 0.07, 0.05], 'n_estimators':[100, 120, 150]}\n",
    "gsv_clf = GridSearchCV(gbc, parameters, cv=5, verbose=1, scoring=scoring, refit='roc_auc', return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3290            3.30m\n",
      "         2           1.2814            1.87m\n",
      "         3           1.2424            1.37m\n",
      "         4           1.2085            1.09m\n",
      "         5           1.1797           55.12s\n",
      "         6           1.1548           48.37s\n",
      "         7           1.1336           44.77s\n",
      "         8           1.1142           40.25s\n",
      "         9           1.0976           39.27s\n",
      "        10           1.0823           37.65s\n",
      "        20           0.9885           21.92s\n",
      "        30           0.9384           15.48s\n",
      "        40           0.9042           11.64s\n",
      "        50           0.8763            9.04s\n",
      "        60           0.8540            6.63s\n",
      "        70           0.8331            4.70s\n",
      "        80           0.8146            2.99s\n",
      "        90           0.7983            1.43s\n",
      "       100           0.7848            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3300           10.00s\n",
      "         2           1.2847            9.28s\n",
      "         3           1.2460            8.98s\n",
      "         4           1.2137            8.82s\n",
      "         5           1.1853            8.71s\n",
      "         6           1.1613            8.59s\n",
      "         7           1.1396            8.47s\n",
      "         8           1.1206            8.38s\n",
      "         9           1.1045            8.28s\n",
      "        10           1.0899            8.18s\n",
      "        20           1.0002            7.16s\n",
      "        30           0.9499            6.22s\n",
      "        40           0.9185            5.30s\n",
      "        50           0.8920            4.42s\n",
      "        60           0.8681            3.54s\n",
      "        70           0.8487            2.64s\n",
      "        80           0.8314            1.76s\n",
      "        90           0.8144            0.88s\n",
      "       100           0.7977            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3291            8.74s\n",
      "         2           1.2824            8.41s\n",
      "         3           1.2436            8.41s\n",
      "         4           1.2104            8.44s\n",
      "         5           1.1814            8.40s\n",
      "         6           1.1570            8.33s\n",
      "         7           1.1355            8.26s\n",
      "         8           1.1169            8.18s\n",
      "         9           1.1000            8.12s\n",
      "        10           1.0844            8.05s\n",
      "        20           0.9922            7.10s\n",
      "        30           0.9431            6.19s\n",
      "        40           0.9081            5.29s\n",
      "        50           0.8808            4.40s\n",
      "        60           0.8584            3.54s\n",
      "        70           0.8365            2.65s\n",
      "        80           0.8188            1.77s\n",
      "        90           0.8023            0.88s\n",
      "       100           0.7887            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3310            8.75s\n",
      "         2           1.2857            8.40s\n",
      "         3           1.2478            8.47s\n",
      "         4           1.2156            8.47s\n",
      "         5           1.1878            8.45s\n",
      "         6           1.1636            8.38s\n",
      "         7           1.1424            8.32s\n",
      "         8           1.1237            8.25s\n",
      "         9           1.1073            8.18s\n",
      "        10           1.0925            8.10s\n",
      "        20           0.9987            7.12s\n",
      "        30           0.9517            6.21s\n",
      "        40           0.9182            5.30s\n",
      "        50           0.8904            4.41s\n",
      "        60           0.8663            3.52s\n",
      "        70           0.8453            2.64s\n",
      "        80           0.8285            1.76s\n",
      "        90           0.8111            0.88s\n",
      "       100           0.7969            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3300            8.75s\n",
      "         2           1.2836            8.71s\n",
      "         3           1.2449            8.68s\n",
      "         4           1.2118            8.82s\n",
      "         5           1.1832            8.71s\n",
      "         6           1.1584            8.61s\n",
      "         7           1.1370            8.50s\n",
      "         8           1.1189            8.38s\n",
      "         9           1.1022            8.28s\n",
      "        10           1.0871            8.20s\n",
      "        20           0.9931            7.17s\n",
      "        30           0.9425            6.30s\n",
      "        40           0.9057            5.36s\n",
      "        50           0.8765            4.44s\n",
      "        60           0.8527            3.54s\n",
      "        70           0.8326            2.65s\n",
      "        80           0.8143            1.76s\n",
      "        90           0.7978            0.88s\n",
      "       100           0.7823            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3290           11.13s\n",
      "         2           1.2814           10.90s\n",
      "         3           1.2424           10.81s\n",
      "         4           1.2085           10.66s\n",
      "         5           1.1797           10.57s\n",
      "         6           1.1548           10.46s\n",
      "         7           1.1336           10.38s\n",
      "         8           1.1142           10.28s\n",
      "         9           1.0976           10.17s\n",
      "        10           1.0823           10.07s\n",
      "        20           0.9885            9.04s\n",
      "        30           0.9384            8.06s\n",
      "        40           0.9042            7.12s\n",
      "        50           0.8763            6.21s\n",
      "        60           0.8540            5.30s\n",
      "        70           0.8331            4.42s\n",
      "        80           0.8146            3.52s\n",
      "        90           0.7983            2.64s\n",
      "       100           0.7848            1.76s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3300           11.18s\n",
      "         2           1.2847           10.75s\n",
      "         3           1.2460           10.87s\n",
      "         4           1.2137           10.78s\n",
      "         5           1.1853           10.62s\n",
      "         6           1.1613           10.50s\n",
      "         7           1.1396           10.57s\n",
      "         8           1.1206           10.50s\n",
      "         9           1.1045           10.35s\n",
      "        10           1.0899           10.25s\n",
      "        20           1.0002            9.07s\n",
      "        30           0.9499            8.09s\n",
      "        40           0.9185            7.13s\n",
      "        50           0.8920            6.21s\n",
      "        60           0.8681            5.31s\n",
      "        70           0.8487            4.41s\n",
      "        80           0.8314            3.52s\n",
      "        90           0.8144            2.64s\n",
      "       100           0.7977            1.76s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3291           10.63s\n",
      "         2           1.2824           10.38s\n",
      "         3           1.2436           10.48s\n",
      "         4           1.2104           10.58s\n",
      "         5           1.1814           10.44s\n",
      "         6           1.1570           10.34s\n",
      "         7           1.1355           10.40s\n",
      "         8           1.1169           10.25s\n",
      "         9           1.1000           10.14s\n",
      "        10           1.0844           10.06s\n",
      "        20           0.9922            9.00s\n",
      "        30           0.9431            8.05s\n",
      "        40           0.9081            7.12s\n",
      "        50           0.8808            6.21s\n",
      "        60           0.8584            5.31s\n",
      "        70           0.8365            4.42s\n",
      "        80           0.8188            3.53s\n",
      "        90           0.8023            2.65s\n",
      "       100           0.7887            1.76s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3310           10.52s\n",
      "         2           1.2857           10.07s\n",
      "         3           1.2478           10.16s\n",
      "         4           1.2156           10.20s\n",
      "         5           1.1878           10.15s\n",
      "         6           1.1636           10.10s\n",
      "         7           1.1424           10.04s\n",
      "         8           1.1237            9.96s\n",
      "         9           1.1073            9.88s\n",
      "        10           1.0925            9.81s\n",
      "        20           0.9987            8.85s\n",
      "        30           0.9517            7.94s\n",
      "        40           0.9182            7.03s\n",
      "        50           0.8904            6.14s\n",
      "        60           0.8663            5.26s\n",
      "        70           0.8453            4.38s\n",
      "        80           0.8285            3.50s\n",
      "        90           0.8111            2.62s\n",
      "       100           0.7969            1.74s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3300           10.51s\n",
      "         2           1.2836           10.32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         3           1.2449           10.30s\n",
      "         4           1.2118           10.26s\n",
      "         5           1.1832           10.19s\n",
      "         6           1.1584           10.15s\n",
      "         7           1.1370           10.08s\n",
      "         8           1.1189            9.97s\n",
      "         9           1.1022            9.89s\n",
      "        10           1.0871            9.82s\n",
      "        20           0.9931            8.85s\n",
      "        30           0.9425            7.93s\n",
      "        40           0.9057            7.02s\n",
      "        50           0.8765            6.13s\n",
      "        60           0.8527            5.25s\n",
      "        70           0.8326            4.36s\n",
      "        80           0.8143            3.49s\n",
      "        90           0.7978            2.61s\n",
      "       100           0.7823            1.74s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3290           13.94s\n",
      "         2           1.2814           13.52s\n",
      "         3           1.2424           13.41s\n",
      "         4           1.2085           13.33s\n",
      "         5           1.1797           13.19s\n",
      "         6           1.1548           13.08s\n",
      "         7           1.1336           12.99s\n",
      "         8           1.1142           12.88s\n",
      "         9           1.0976           12.76s\n",
      "        10           1.0823           12.68s\n",
      "        20           0.9885           11.62s\n",
      "        30           0.9384           10.66s\n",
      "        40           0.9042            9.73s\n",
      "        50           0.8763            8.82s\n",
      "        60           0.8540            7.91s\n",
      "        70           0.8331            7.04s\n",
      "        80           0.8146            6.16s\n",
      "        90           0.7983            5.27s\n",
      "       100           0.7848            4.39s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3300           14.31s\n",
      "         2           1.2847           13.81s\n",
      "         3           1.2460           13.59s\n",
      "         4           1.2137           13.50s\n",
      "         5           1.1853           13.34s\n",
      "         6           1.1613           13.24s\n",
      "         7           1.1396           13.19s\n",
      "         8           1.1206           13.05s\n",
      "         9           1.1045           12.94s\n",
      "        10           1.0899           12.89s\n",
      "        20           1.0002           11.89s\n",
      "        30           0.9499           10.93s\n",
      "        40           0.9185            9.92s\n",
      "        50           0.8920            8.97s\n",
      "        60           0.8681            8.05s\n",
      "        70           0.8487            7.13s\n",
      "        80           0.8314            6.22s\n",
      "        90           0.8144            5.32s\n",
      "       100           0.7977            4.43s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3291           13.22s\n",
      "         2           1.2824           12.62s\n",
      "         3           1.2436           12.68s\n",
      "         4           1.2104           12.82s\n",
      "         5           1.1814           12.75s\n",
      "         6           1.1570           12.74s\n",
      "         7           1.1355           12.70s\n",
      "         8           1.1169           12.61s\n",
      "         9           1.1000           12.54s\n",
      "        10           1.0844           12.48s\n",
      "        20           0.9922           11.53s\n",
      "        30           0.9431           10.64s\n",
      "        40           0.9081            9.72s\n",
      "        50           0.8808            8.81s\n",
      "        60           0.8584            7.92s\n",
      "        70           0.8365            7.03s\n",
      "        80           0.8188            6.15s\n",
      "        90           0.8023            5.27s\n",
      "       100           0.7887            4.38s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3310           13.23s\n",
      "         2           1.2857           12.64s\n",
      "         3           1.2478           12.78s\n",
      "         4           1.2156           12.86s\n",
      "         5           1.1878           12.79s\n",
      "         6           1.1636           12.75s\n",
      "         7           1.1424           12.71s\n",
      "         8           1.1237           12.62s\n",
      "         9           1.1073           12.55s\n",
      "        10           1.0925           12.46s\n",
      "        20           0.9987           11.50s\n",
      "        30           0.9517           10.58s\n",
      "        40           0.9182            9.67s\n",
      "        50           0.8904            8.78s\n",
      "        60           0.8663            7.91s\n",
      "        70           0.8453            7.02s\n",
      "        80           0.8285            6.12s\n",
      "        90           0.8111            5.24s\n",
      "       100           0.7969            4.36s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3300           13.09s\n",
      "         2           1.2836           12.86s\n",
      "         3           1.2449           12.92s\n",
      "         4           1.2118           13.07s\n",
      "         5           1.1831           13.00s\n",
      "         6           1.1584           12.91s\n",
      "         7           1.1369           12.83s\n",
      "         8           1.1187           12.67s\n",
      "         9           1.1020           12.58s\n",
      "        10           1.0870           12.50s\n",
      "        20           0.9930           11.47s\n",
      "        30           0.9428           10.53s\n",
      "        40           0.9071            9.62s\n",
      "        50           0.8785            8.71s\n",
      "        60           0.8543            7.83s\n",
      "        70           0.8338            6.95s\n",
      "        80           0.8159            6.07s\n",
      "        90           0.7997            5.20s\n",
      "       100           0.7842            4.33s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3456            9.22s\n",
      "         2           1.3098            8.90s\n",
      "         3           1.2785            8.80s\n",
      "         4           1.2510            8.69s\n",
      "         5           1.2262            8.55s\n",
      "         6           1.2040            8.46s\n",
      "         7           1.1840            8.35s\n",
      "         8           1.1661            8.24s\n",
      "         9           1.1502            8.14s\n",
      "        10           1.1353            8.06s\n",
      "        20           1.0358            7.10s\n",
      "        30           0.9830            6.19s\n",
      "        40           0.9462            5.30s\n",
      "        50           0.9198            4.40s\n",
      "        60           0.8983            3.51s\n",
      "        70           0.8806            2.63s\n",
      "        80           0.8628            1.75s\n",
      "        90           0.8486            0.87s\n",
      "       100           0.8355            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3463            9.25s\n",
      "         2           1.3121            8.87s\n",
      "         3           1.2815            8.75s\n",
      "         4           1.2549            8.65s\n",
      "         5           1.2305            8.52s\n",
      "         6           1.2088            8.43s\n",
      "         7           1.1895            8.33s\n",
      "         8           1.1718            8.24s\n",
      "         9           1.1559            8.16s\n",
      "        10           1.1411            8.06s\n",
      "        20           1.0454            7.08s\n",
      "        30           0.9939            6.19s\n",
      "        40           0.9587            5.29s\n",
      "        50           0.9318            4.39s\n",
      "        60           0.9112            3.51s\n",
      "        70           0.8926            2.62s\n",
      "        80           0.8753            1.75s\n",
      "        90           0.8603            0.87s\n",
      "       100           0.8469            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3456            8.74s\n",
      "         2           1.3103            8.38s\n",
      "         3           1.2794            8.40s\n",
      "         4           1.2522            8.39s\n",
      "         5           1.2277            8.32s\n",
      "         6           1.2056            8.27s\n",
      "         7           1.1860            8.18s\n",
      "         8           1.1680            8.11s\n",
      "         9           1.1518            8.04s\n",
      "        10           1.1371            7.96s\n",
      "        20           1.0403            7.04s\n",
      "        30           0.9874            6.16s\n",
      "        40           0.9525            5.28s\n",
      "        50           0.9247            4.39s\n",
      "        60           0.9028            3.51s\n",
      "        70           0.8847            2.62s\n",
      "        80           0.8681            1.75s\n",
      "        90           0.8522            0.87s\n",
      "       100           0.8383            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3470            8.71s\n",
      "         2           1.3127            8.40s\n",
      "         3           1.2826            8.44s\n",
      "         4           1.2562            8.36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         5           1.2324            8.31s\n",
      "         6           1.2111            8.28s\n",
      "         7           1.1915            8.21s\n",
      "         8           1.1743            8.13s\n",
      "         9           1.1582            8.07s\n",
      "        10           1.1440            7.97s\n",
      "        20           1.0470            7.04s\n",
      "        30           0.9960            6.14s\n",
      "        40           0.9598            5.26s\n",
      "        50           0.9327            4.38s\n",
      "        60           0.9115            3.50s\n",
      "        70           0.8922            2.62s\n",
      "        80           0.8741            1.75s\n",
      "        90           0.8588            0.87s\n",
      "       100           0.8457            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3463            8.72s\n",
      "         2           1.3114            8.61s\n",
      "         3           1.2806            8.63s\n",
      "         4           1.2533            8.49s\n",
      "         5           1.2292            8.43s\n",
      "         6           1.2072            8.34s\n",
      "         7           1.1874            8.25s\n",
      "         8           1.1694            8.16s\n",
      "         9           1.1532            8.09s\n",
      "        10           1.1386            7.99s\n",
      "        20           1.0407            7.04s\n",
      "        30           0.9873            6.15s\n",
      "        40           0.9519            5.26s\n",
      "        50           0.9234            4.37s\n",
      "        60           0.9005            3.49s\n",
      "        70           0.8811            2.61s\n",
      "        80           0.8638            1.74s\n",
      "        90           0.8483            0.87s\n",
      "       100           0.8340            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3456           11.08s\n",
      "         2           1.3098           10.80s\n",
      "         3           1.2785           10.67s\n",
      "         4           1.2510           10.45s\n",
      "         5           1.2262           10.30s\n",
      "         6           1.2040           10.24s\n",
      "         7           1.1840           10.10s\n",
      "         8           1.1661           10.02s\n",
      "         9           1.1502            9.94s\n",
      "        10           1.1353            9.84s\n",
      "        20           1.0358            8.86s\n",
      "        30           0.9830            7.95s\n",
      "        40           0.9462            7.06s\n",
      "        50           0.9198            6.15s\n",
      "        60           0.8983            5.26s\n",
      "        70           0.8806            4.38s\n",
      "        80           0.8628            3.50s\n",
      "        90           0.8486            2.62s\n",
      "       100           0.8355            1.75s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3463           11.16s\n",
      "         2           1.3121           10.68s\n",
      "         3           1.2815           10.53s\n",
      "         4           1.2549           10.47s\n",
      "         5           1.2305           10.32s\n",
      "         6           1.2088           10.20s\n",
      "         7           1.1895           10.13s\n",
      "         8           1.1718           10.02s\n",
      "         9           1.1559            9.93s\n",
      "        10           1.1411            9.85s\n",
      "        20           1.0454            8.88s\n",
      "        30           0.9939            7.96s\n",
      "        40           0.9587            7.06s\n",
      "        50           0.9318            6.15s\n",
      "        60           0.9112            5.27s\n",
      "        70           0.8926            4.38s\n",
      "        80           0.8753            3.50s\n",
      "        90           0.8603            2.62s\n",
      "       100           0.8469            1.75s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3456           10.53s\n",
      "         2           1.3103           10.05s\n",
      "         3           1.2794           10.61s\n",
      "         4           1.2522           10.63s\n",
      "         5           1.2277           10.51s\n",
      "         6           1.2056           10.50s\n",
      "         7           1.1860           10.52s\n",
      "         8           1.1680           10.40s\n",
      "         9           1.1518           10.43s\n",
      "        10           1.1371           10.41s\n",
      "        20           1.0403            9.35s\n",
      "        30           0.9874            8.38s\n",
      "        40           0.9525            7.40s\n",
      "        50           0.9247            6.46s\n",
      "        60           0.9028            5.53s\n",
      "        70           0.8847            4.59s\n",
      "        80           0.8681            3.66s\n",
      "        90           0.8522            2.74s\n",
      "       100           0.8383            1.82s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3470           10.57s\n",
      "         2           1.3127           10.08s\n",
      "         3           1.2826           10.10s\n",
      "         4           1.2562           10.23s\n",
      "         5           1.2324           10.16s\n",
      "         6           1.2111           10.11s\n",
      "         7           1.1915           10.07s\n",
      "         8           1.1743            9.99s\n",
      "         9           1.1582            9.91s\n",
      "        10           1.1440            9.82s\n",
      "        20           1.0470            8.85s\n",
      "        30           0.9960            7.94s\n",
      "        40           0.9598            7.05s\n",
      "        50           0.9327            6.16s\n",
      "        60           0.9115            5.30s\n",
      "        70           0.8922            4.43s\n",
      "        80           0.8741            3.55s\n",
      "        90           0.8588            2.66s\n",
      "       100           0.8457            1.77s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3463           10.59s\n",
      "         2           1.3114           10.32s\n",
      "         3           1.2806           10.31s\n",
      "         4           1.2533           10.26s\n",
      "         5           1.2292           10.16s\n",
      "         6           1.2072           10.07s\n",
      "         7           1.1874           10.03s\n",
      "         8           1.1694            9.93s\n",
      "         9           1.1532            9.86s\n",
      "        10           1.1386            9.79s\n",
      "        20           1.0407            8.83s\n",
      "        30           0.9873            7.92s\n",
      "        40           0.9519            7.04s\n",
      "        50           0.9234            6.15s\n",
      "        60           0.9005            5.26s\n",
      "        70           0.8811            4.38s\n",
      "        80           0.8638            3.49s\n",
      "        90           0.8483            2.62s\n",
      "       100           0.8340            1.74s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3456           13.93s\n",
      "         2           1.3098           13.47s\n",
      "         3           1.2785           13.38s\n",
      "         4           1.2510           13.24s\n",
      "         5           1.2262           13.07s\n",
      "         6           1.2040           13.04s\n",
      "         7           1.1840           12.96s\n",
      "         8           1.1661           12.88s\n",
      "         9           1.1502           12.79s\n",
      "        10           1.1353           12.72s\n",
      "        20           1.0358           11.83s\n",
      "        30           0.9830           11.05s\n",
      "        40           0.9462           10.04s\n",
      "        50           0.9198            9.06s\n",
      "        60           0.8983            8.11s\n",
      "        70           0.8806            7.17s\n",
      "        80           0.8628            6.26s\n",
      "        90           0.8486            5.35s\n",
      "       100           0.8355            4.45s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3463           14.12s\n",
      "         2           1.3121           13.53s\n",
      "         3           1.2815           13.38s\n",
      "         4           1.2549           13.22s\n",
      "         5           1.2305           13.05s\n",
      "         6           1.2088           12.91s\n",
      "         7           1.1895           12.80s\n",
      "         8           1.1718           12.71s\n",
      "         9           1.1559           12.61s\n",
      "        10           1.1411           12.52s\n",
      "        20           1.0454           11.54s\n",
      "        30           0.9939           10.66s\n",
      "        40           0.9587            9.76s\n",
      "        50           0.9318            8.85s\n",
      "        60           0.9112            7.95s\n",
      "        70           0.8926            7.05s\n",
      "        80           0.8753            6.17s\n",
      "        90           0.8603            5.28s\n",
      "       100           0.8469            4.40s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3456           13.46s\n",
      "         2           1.3103           12.83s\n",
      "         3           1.2794           12.82s\n",
      "         4           1.2522           12.83s\n",
      "         5           1.2277           12.75s\n",
      "         6           1.2056           12.71s\n",
      "         7           1.1860           12.64s\n",
      "         8           1.1680           12.57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         9           1.1518           12.53s\n",
      "        10           1.1371           12.51s\n",
      "        20           1.0403           11.61s\n",
      "        30           0.9874           10.72s\n",
      "        40           0.9525            9.84s\n",
      "        50           0.9247            8.92s\n",
      "        60           0.9028            8.02s\n",
      "        70           0.8847            7.13s\n",
      "        80           0.8681            6.24s\n",
      "        90           0.8522            5.38s\n",
      "       100           0.8383            4.48s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3470           13.23s\n",
      "         2           1.3127           12.65s\n",
      "         3           1.2826           17.11s\n",
      "         4           1.2562           16.08s\n",
      "         5           1.2324           15.43s\n",
      "         6           1.2111           14.92s\n",
      "         7           1.1915           14.54s\n",
      "         8           1.1743           14.22s\n",
      "         9           1.1582           13.98s\n",
      "        10           1.1440           13.76s\n",
      "        20           1.0470           12.57s\n",
      "        30           0.9960           11.44s\n",
      "        40           0.9598           10.30s\n",
      "        50           0.9327            9.24s\n",
      "        60           0.9115            8.23s\n",
      "        70           0.8922            7.28s\n",
      "        80           0.8741            6.33s\n",
      "        90           0.8588            5.41s\n",
      "       100           0.8457            4.49s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3463           13.18s\n",
      "         2           1.3114           13.25s\n",
      "         3           1.2806           13.44s\n",
      "         4           1.2533           13.19s\n",
      "         5           1.2292           13.06s\n",
      "         6           1.2072           13.00s\n",
      "         7           1.1874           12.87s\n",
      "         8           1.1694           12.76s\n",
      "         9           1.1532           12.72s\n",
      "        10           1.1386           12.60s\n",
      "        20           1.0407           11.61s\n",
      "        30           0.9873           10.65s\n",
      "        40           0.9519            9.74s\n",
      "        50           0.9234            8.82s\n",
      "        60           0.9005            7.91s\n",
      "        70           0.8811            7.01s\n",
      "        80           0.8638            6.15s\n",
      "        90           0.8483            5.27s\n",
      "       100           0.8340            4.40s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3569            9.43s\n",
      "         2           1.3301            9.17s\n",
      "         3           1.3058            8.91s\n",
      "         4           1.2835            8.73s\n",
      "         5           1.2633            8.57s\n",
      "         6           1.2446            8.45s\n",
      "         7           1.2272            8.32s\n",
      "         8           1.2113            8.23s\n",
      "         9           1.1964            8.12s\n",
      "        10           1.1825            8.03s\n",
      "        20           1.0847            7.09s\n",
      "        30           1.0270            6.18s\n",
      "        40           0.9897            5.29s\n",
      "        50           0.9625            4.41s\n",
      "        60           0.9394            3.52s\n",
      "        70           0.9216            2.63s\n",
      "        80           0.9063            1.75s\n",
      "        90           0.8921            0.88s\n",
      "       100           0.8788            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3574            9.26s\n",
      "         2           1.3318            9.44s\n",
      "         3           1.3079            9.14s\n",
      "         4           1.2862            8.88s\n",
      "         5           1.2666            8.81s\n",
      "         6           1.2482            8.71s\n",
      "         7           1.2315            8.54s\n",
      "         8           1.2157            8.41s\n",
      "         9           1.2013            8.26s\n",
      "        10           1.1877            8.15s\n",
      "        20           1.0918            7.15s\n",
      "        30           1.0370            6.21s\n",
      "        40           1.0007            5.31s\n",
      "        50           0.9740            4.44s\n",
      "        60           0.9517            3.54s\n",
      "        70           0.9348            2.65s\n",
      "        80           0.9194            1.77s\n",
      "        90           0.9055            0.88s\n",
      "       100           0.8934            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3570            9.36s\n",
      "         2           1.3304            8.92s\n",
      "         3           1.3064            9.71s\n",
      "         4           1.2844           10.09s\n",
      "         5           1.2644            9.90s\n",
      "         6           1.2458            9.59s\n",
      "         7           1.2286            9.35s\n",
      "         8           1.2128            9.12s\n",
      "         9           1.1980            8.91s\n",
      "        10           1.1843            8.75s\n",
      "        20           1.0872            7.52s\n",
      "        30           1.0309            6.47s\n",
      "        40           0.9944            5.57s\n",
      "        50           0.9671            4.79s\n",
      "        60           0.9448            3.87s\n",
      "        70           0.9259            2.90s\n",
      "        80           0.9100            1.93s\n",
      "        90           0.8959            0.96s\n",
      "       100           0.8831            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3579            9.14s\n",
      "         2           1.3322            8.79s\n",
      "         3           1.3089            8.67s\n",
      "         4           1.2874            8.67s\n",
      "         5           1.2680            8.73s\n",
      "         6           1.2500            8.77s\n",
      "         7           1.2334            8.72s\n",
      "         8           1.2179            8.67s\n",
      "         9           1.2035            8.57s\n",
      "        10           1.1903            8.46s\n",
      "        20           1.0946            7.39s\n",
      "        30           1.0386            6.35s\n",
      "        40           1.0016            5.44s\n",
      "        50           0.9745            4.52s\n",
      "        60           0.9521            3.62s\n",
      "        70           0.9336            2.71s\n",
      "        80           0.9183            1.79s\n",
      "        90           0.9030            0.89s\n",
      "       100           0.8897            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3574            8.80s\n",
      "         2           1.3313            8.62s\n",
      "         3           1.3073            8.55s\n",
      "         4           1.2856            8.45s\n",
      "         5           1.2656            8.40s\n",
      "         6           1.2470            8.33s\n",
      "         7           1.2298            8.25s\n",
      "         8           1.2141            8.14s\n",
      "         9           1.1992            8.05s\n",
      "        10           1.1856            7.96s\n",
      "        20           1.0889            7.05s\n",
      "        30           1.0322            6.18s\n",
      "        40           0.9947            5.32s\n",
      "        50           0.9671            4.46s\n",
      "        60           0.9439            3.58s\n",
      "        70           0.9244            2.69s\n",
      "        80           0.9067            1.80s\n",
      "        90           0.8917            0.90s\n",
      "       100           0.8782            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3569           11.85s\n",
      "         2           1.3301           11.40s\n",
      "         3           1.3058           11.11s\n",
      "         4           1.2835           11.01s\n",
      "         5           1.2633           10.88s\n",
      "         6           1.2446           10.80s\n",
      "         7           1.2272           10.67s\n",
      "         8           1.2113           10.57s\n",
      "         9           1.1964           10.47s\n",
      "        10           1.1825           10.36s\n",
      "        20           1.0847            9.31s\n",
      "        30           1.0270            8.29s\n",
      "        40           0.9897            7.29s\n",
      "        50           0.9625            6.38s\n",
      "        60           0.9394            5.46s\n",
      "        70           0.9216            4.53s\n",
      "        80           0.9063            3.60s\n",
      "        90           0.8921            2.70s\n",
      "       100           0.8788            1.80s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3574           11.14s\n",
      "         2           1.3318           10.68s\n",
      "         3           1.3079           10.99s\n",
      "         4           1.2862           10.76s\n",
      "         5           1.2666           10.60s\n",
      "         6           1.2482           10.60s\n",
      "         7           1.2315           10.42s\n",
      "         8           1.2157           10.29s\n",
      "         9           1.2013           10.18s\n",
      "        10           1.1877           10.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        20           1.0918            9.15s\n",
      "        30           1.0370            8.25s\n",
      "        40           1.0007            7.25s\n",
      "        50           0.9740            6.35s\n",
      "        60           0.9517            5.44s\n",
      "        70           0.9348            4.52s\n",
      "        80           0.9194            3.61s\n",
      "        90           0.9055            2.71s\n",
      "       100           0.8934            1.81s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3570           10.80s\n",
      "         2           1.3304           10.19s\n",
      "         3           1.3064           10.06s\n",
      "         4           1.2844            9.99s\n",
      "         5           1.2644            9.95s\n",
      "         6           1.2458            9.87s\n",
      "         7           1.2286            9.84s\n",
      "         8           1.2128            9.84s\n",
      "         9           1.1980            9.81s\n",
      "        10           1.1843            9.76s\n",
      "        20           1.0872            8.99s\n",
      "        30           1.0309            8.11s\n",
      "        40           0.9944            7.22s\n",
      "        50           0.9671            6.32s\n",
      "        60           0.9448            5.42s\n",
      "        70           0.9259            4.51s\n",
      "        80           0.9100            3.61s\n",
      "        90           0.8959            2.71s\n",
      "       100           0.8831            1.81s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3579           11.19s\n",
      "         2           1.3322           10.59s\n",
      "         3           1.3089           10.33s\n",
      "         4           1.2874           10.33s\n",
      "         5           1.2680           10.32s\n",
      "         6           1.2500           10.25s\n",
      "         7           1.2334           10.21s\n",
      "         8           1.2179           10.16s\n",
      "         9           1.2035           10.08s\n",
      "        10           1.1903           10.00s\n",
      "        20           1.0946            9.09s\n",
      "        30           1.0386            8.17s\n",
      "        40           1.0016            7.26s\n",
      "        50           0.9745            6.35s\n",
      "        60           0.9521            5.44s\n",
      "        70           0.9336            4.54s\n",
      "        80           0.9183            3.62s\n",
      "        90           0.9030            2.72s\n",
      "       100           0.8897            1.81s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3574           11.06s\n",
      "         2           1.3313           10.85s\n",
      "         3           1.3073           10.74s\n",
      "         4           1.2856           10.67s\n",
      "         5           1.2656           10.55s\n",
      "         6           1.2470           10.48s\n",
      "         7           1.2298           10.38s\n",
      "         8           1.2141           10.27s\n",
      "         9           1.1992           10.18s\n",
      "        10           1.1856           10.08s\n",
      "        20           1.0889            9.11s\n",
      "        30           1.0322            8.19s\n",
      "        40           0.9947            7.27s\n",
      "        50           0.9671            6.36s\n",
      "        60           0.9439            5.44s\n",
      "        70           0.9244            4.53s\n",
      "        80           0.9067            3.63s\n",
      "        90           0.8917            2.72s\n",
      "       100           0.8782            1.81s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3569           14.83s\n",
      "         2           1.3301           14.30s\n",
      "         3           1.3058           14.09s\n",
      "         4           1.2835           14.01s\n",
      "         5           1.2633           13.83s\n",
      "         6           1.2446           13.77s\n",
      "         7           1.2272           13.64s\n",
      "         8           1.2113           13.52s\n",
      "         9           1.1964           13.40s\n",
      "        10           1.1825           13.29s\n",
      "        20           1.0847           12.13s\n",
      "        30           1.0270           11.15s\n",
      "        40           0.9897           10.11s\n",
      "        50           0.9625            9.11s\n",
      "        60           0.9394            8.13s\n",
      "        70           0.9216            7.18s\n",
      "        80           0.9063            6.26s\n",
      "        90           0.8921            5.34s\n",
      "       100           0.8788            4.44s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3574           14.21s\n",
      "         2           1.3318           13.73s\n",
      "         3           1.3079           13.60s\n",
      "         4           1.2862           13.40s\n",
      "         5           1.2666           13.26s\n",
      "         6           1.2482           13.14s\n",
      "         7           1.2315           13.02s\n",
      "         8           1.2157           12.93s\n",
      "         9           1.2013           12.81s\n",
      "        10           1.1877           12.80s\n",
      "        20           1.0918           11.94s\n",
      "        30           1.0370           10.90s\n",
      "        40           1.0007            9.94s\n",
      "        50           0.9740            9.00s\n",
      "        60           0.9517            8.06s\n",
      "        70           0.9348            7.14s\n",
      "        80           0.9194            6.23s\n",
      "        90           0.9055            5.33s\n",
      "       100           0.8934            4.43s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3570           13.20s\n",
      "         2           1.3304           12.61s\n",
      "         3           1.3064           12.38s\n",
      "         4           1.2844           12.38s\n",
      "         5           1.2644           12.41s\n",
      "         6           1.2458           12.35s\n",
      "         7           1.2286           12.32s\n",
      "         8           1.2128           12.27s\n",
      "         9           1.1980           12.22s\n",
      "        10           1.1843           12.17s\n",
      "        20           1.0872           11.35s\n",
      "        30           1.0309           10.49s\n",
      "        40           0.9944            9.61s\n",
      "        50           0.9671            8.73s\n",
      "        60           0.9448            7.86s\n",
      "        70           0.9259            6.97s\n",
      "        80           0.9100            6.10s\n",
      "        90           0.8959            5.22s\n",
      "       100           0.8831            4.35s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3579           13.13s\n",
      "         2           1.3322           12.56s\n",
      "         3           1.3089           12.35s\n",
      "         4           1.2874           12.38s\n",
      "         5           1.2680           12.36s\n",
      "         6           1.2500           12.34s\n",
      "         7           1.2334           12.31s\n",
      "         8           1.2179           12.26s\n",
      "         9           1.2035           12.21s\n",
      "        10           1.1903           12.17s\n",
      "        20           1.0946           11.43s\n",
      "        30           1.0386           10.62s\n",
      "        40           1.0016            9.75s\n",
      "        50           0.9745            8.83s\n",
      "        60           0.9521            7.93s\n",
      "        70           0.9336            7.03s\n",
      "        80           0.9183            6.14s\n",
      "        90           0.9030            5.25s\n",
      "       100           0.8897            4.37s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3574           13.12s\n",
      "         2           1.3313           12.89s\n",
      "         3           1.3073           12.86s\n",
      "         4           1.2856           12.73s\n",
      "         5           1.2656           12.65s\n",
      "         6           1.2470           12.55s\n",
      "         7           1.2298           12.49s\n",
      "         8           1.2141           12.38s\n",
      "         9           1.1992           12.30s\n",
      "        10           1.1856           12.23s\n",
      "        20           1.0889           11.36s\n",
      "        30           1.0322           10.48s\n",
      "        40           0.9947            9.73s\n",
      "        50           0.9671            8.87s\n",
      "        60           0.9439            7.98s\n",
      "        70           0.9244            7.08s\n",
      "        80           0.9067            6.19s\n",
      "        90           0.8917            5.30s\n",
      "       100           0.8782            4.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3299            2.43m\n",
      "         2           1.2837            1.31m\n",
      "         3           1.2455           56.30s\n",
      "         4           1.2124           45.01s\n",
      "         5           1.1843           38.24s\n",
      "         6           1.1601           33.68s\n",
      "         7           1.1387           30.40s\n",
      "         8           1.1205           27.90s\n",
      "         9           1.1038           25.94s\n",
      "        10           1.0895           24.33s\n",
      "        20           0.9985           16.50s\n",
      "        30           0.9503           13.28s\n",
      "        40           0.9184           11.10s\n",
      "        50           0.8928            9.29s\n",
      "        60           0.8711            7.71s\n",
      "        70           0.8519            6.28s\n",
      "        80           0.8348            4.95s\n",
      "        90           0.8223            3.68s\n",
      "       100           0.8106            2.42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=1,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate': [0.1, 0.07, 0.05], 'n_estimators': [100, 120, 150]},\n",
       "       pre_dispatch='2*n_jobs', refit='roc_auc', return_train_score=False,\n",
       "       scoring=['precision', 'recall', 'average_precision', 'roc_auc'],\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsv_clf.fit(X_resampled_LE, y_resampled_LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GridSearch_scores = pd.DataFrame(gsv_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_average_precision</th>\n",
       "      <th>mean_test_precision</th>\n",
       "      <th>mean_test_recall</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_average_precision</th>\n",
       "      <th>...</th>\n",
       "      <th>split4_test_average_precision</th>\n",
       "      <th>split4_test_precision</th>\n",
       "      <th>split4_test_recall</th>\n",
       "      <th>split4_test_roc_auc</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_average_precision</th>\n",
       "      <th>std_test_precision</th>\n",
       "      <th>std_test_recall</th>\n",
       "      <th>std_test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.029903</td>\n",
       "      <td>0.207216</td>\n",
       "      <td>0.877732</td>\n",
       "      <td>0.824354</td>\n",
       "      <td>0.676747</td>\n",
       "      <td>0.852603</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867505</td>\n",
       "      <td>0.798020</td>\n",
       "      <td>0.677311</td>\n",
       "      <td>0.840878</td>\n",
       "      <td>2.383400</td>\n",
       "      <td>0.316873</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>0.018963</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.011509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.528921</td>\n",
       "      <td>0.054912</td>\n",
       "      <td>0.878028</td>\n",
       "      <td>0.817975</td>\n",
       "      <td>0.679772</td>\n",
       "      <td>0.853199</td>\n",
       "      <td>0.1</td>\n",
       "      <td>120</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 120}</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866440</td>\n",
       "      <td>0.785575</td>\n",
       "      <td>0.677311</td>\n",
       "      <td>0.840113</td>\n",
       "      <td>0.055041</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.011218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.094404</td>\n",
       "      <td>0.064382</td>\n",
       "      <td>0.877565</td>\n",
       "      <td>0.815553</td>\n",
       "      <td>0.682124</td>\n",
       "      <td>0.852594</td>\n",
       "      <td>0.1</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 150}</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865833</td>\n",
       "      <td>0.792233</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.839381</td>\n",
       "      <td>0.098407</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.010990</td>\n",
       "      <td>0.018631</td>\n",
       "      <td>0.008650</td>\n",
       "      <td>0.011124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.741413</td>\n",
       "      <td>0.047819</td>\n",
       "      <td>0.876928</td>\n",
       "      <td>0.832622</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.851710</td>\n",
       "      <td>0.07</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.07, 'n_estimators': 100}</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865344</td>\n",
       "      <td>0.803313</td>\n",
       "      <td>0.652101</td>\n",
       "      <td>0.838083</td>\n",
       "      <td>0.016555</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.021366</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.012208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.576246</td>\n",
       "      <td>0.054496</td>\n",
       "      <td>0.877134</td>\n",
       "      <td>0.825750</td>\n",
       "      <td>0.669691</td>\n",
       "      <td>0.852072</td>\n",
       "      <td>0.07</td>\n",
       "      <td>120</td>\n",
       "      <td>{'learning_rate': 0.07, 'n_estimators': 120}</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866076</td>\n",
       "      <td>0.797546</td>\n",
       "      <td>0.655462</td>\n",
       "      <td>0.838842</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.020441</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.012319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.332615</td>\n",
       "      <td>0.064434</td>\n",
       "      <td>0.877149</td>\n",
       "      <td>0.817104</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.852336</td>\n",
       "      <td>0.07</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.07, 'n_estimators': 150}</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865214</td>\n",
       "      <td>0.789370</td>\n",
       "      <td>0.673950</td>\n",
       "      <td>0.838719</td>\n",
       "      <td>0.112730</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.021321</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>0.011511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.038624</td>\n",
       "      <td>0.050819</td>\n",
       "      <td>0.874881</td>\n",
       "      <td>0.846236</td>\n",
       "      <td>0.640121</td>\n",
       "      <td>0.848812</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 100}</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865511</td>\n",
       "      <td>0.826552</td>\n",
       "      <td>0.648739</td>\n",
       "      <td>0.836558</td>\n",
       "      <td>0.286904</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.011866</td>\n",
       "      <td>0.019756</td>\n",
       "      <td>0.023149</td>\n",
       "      <td>0.011755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.853022</td>\n",
       "      <td>0.060159</td>\n",
       "      <td>0.876486</td>\n",
       "      <td>0.838910</td>\n",
       "      <td>0.650874</td>\n",
       "      <td>0.850821</td>\n",
       "      <td>0.05</td>\n",
       "      <td>120</td>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 120}</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867079</td>\n",
       "      <td>0.825902</td>\n",
       "      <td>0.653782</td>\n",
       "      <td>0.839079</td>\n",
       "      <td>0.057710</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>0.012203</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>0.017189</td>\n",
       "      <td>0.011927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.295146</td>\n",
       "      <td>0.064613</td>\n",
       "      <td>0.877300</td>\n",
       "      <td>0.829886</td>\n",
       "      <td>0.661962</td>\n",
       "      <td>0.852227</td>\n",
       "      <td>0.05</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 150}</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866747</td>\n",
       "      <td>0.811715</td>\n",
       "      <td>0.652101</td>\n",
       "      <td>0.839538</td>\n",
       "      <td>0.353545</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.012339</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.012151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_average_precision  \\\n",
       "0      10.029903         0.207216                     0.877732   \n",
       "1      10.528921         0.054912                     0.878028   \n",
       "2      13.094404         0.064382                     0.877565   \n",
       "3       8.741413         0.047819                     0.876928   \n",
       "4      10.576246         0.054496                     0.877134   \n",
       "5      13.332615         0.064434                     0.877149   \n",
       "6       9.038624         0.050819                     0.874881   \n",
       "7      10.853022         0.060159                     0.876486   \n",
       "8      13.295146         0.064613                     0.877300   \n",
       "\n",
       "   mean_test_precision  mean_test_recall  mean_test_roc_auc  \\\n",
       "0             0.824354          0.676747           0.852603   \n",
       "1             0.817975          0.679772           0.853199   \n",
       "2             0.815553          0.682124           0.852594   \n",
       "3             0.832622          0.661290           0.851710   \n",
       "4             0.825750          0.669691           0.852072   \n",
       "5             0.817104          0.677419           0.852336   \n",
       "6             0.846236          0.640121           0.848812   \n",
       "7             0.838910          0.650874           0.850821   \n",
       "8             0.829886          0.661962           0.852227   \n",
       "\n",
       "  param_learning_rate param_n_estimators  \\\n",
       "0                 0.1                100   \n",
       "1                 0.1                120   \n",
       "2                 0.1                150   \n",
       "3                0.07                100   \n",
       "4                0.07                120   \n",
       "5                0.07                150   \n",
       "6                0.05                100   \n",
       "7                0.05                120   \n",
       "8                0.05                150   \n",
       "\n",
       "                                         params  rank_test_average_precision  \\\n",
       "0   {'learning_rate': 0.1, 'n_estimators': 100}                            2   \n",
       "1   {'learning_rate': 0.1, 'n_estimators': 120}                            1   \n",
       "2   {'learning_rate': 0.1, 'n_estimators': 150}                            3   \n",
       "3  {'learning_rate': 0.07, 'n_estimators': 100}                            7   \n",
       "4  {'learning_rate': 0.07, 'n_estimators': 120}                            6   \n",
       "5  {'learning_rate': 0.07, 'n_estimators': 150}                            5   \n",
       "6  {'learning_rate': 0.05, 'n_estimators': 100}                            9   \n",
       "7  {'learning_rate': 0.05, 'n_estimators': 120}                            8   \n",
       "8  {'learning_rate': 0.05, 'n_estimators': 150}                            4   \n",
       "\n",
       "         ...         split4_test_average_precision  split4_test_precision  \\\n",
       "0        ...                              0.867505               0.798020   \n",
       "1        ...                              0.866440               0.785575   \n",
       "2        ...                              0.865833               0.792233   \n",
       "3        ...                              0.865344               0.803313   \n",
       "4        ...                              0.866076               0.797546   \n",
       "5        ...                              0.865214               0.789370   \n",
       "6        ...                              0.865511               0.826552   \n",
       "7        ...                              0.867079               0.825902   \n",
       "8        ...                              0.866747               0.811715   \n",
       "\n",
       "   split4_test_recall  split4_test_roc_auc  std_fit_time  std_score_time  \\\n",
       "0            0.677311             0.840878      2.383400        0.316873   \n",
       "1            0.677311             0.840113      0.055041        0.000379   \n",
       "2            0.685714             0.839381      0.098407        0.001444   \n",
       "3            0.652101             0.838083      0.016555        0.000172   \n",
       "4            0.655462             0.838842      0.150538        0.000390   \n",
       "5            0.673950             0.838719      0.112730        0.000951   \n",
       "6            0.648739             0.836558      0.286904        0.003449   \n",
       "7            0.653782             0.839079      0.057710        0.003076   \n",
       "8            0.652101             0.839538      0.353545        0.000573   \n",
       "\n",
       "   std_test_average_precision  std_test_precision  std_test_recall  \\\n",
       "0                    0.011097            0.018963         0.008080   \n",
       "1                    0.011165            0.019387         0.011986   \n",
       "2                    0.010990            0.018631         0.008650   \n",
       "3                    0.012276            0.021366         0.014763   \n",
       "4                    0.012185            0.020441         0.012186   \n",
       "5                    0.011557            0.021321         0.013247   \n",
       "6                    0.011866            0.019756         0.023149   \n",
       "7                    0.012203            0.019041         0.017189   \n",
       "8                    0.012339            0.020846         0.015115   \n",
       "\n",
       "   std_test_roc_auc  \n",
       "0          0.011509  \n",
       "1          0.011218  \n",
       "2          0.011124  \n",
       "3          0.012208  \n",
       "4          0.012319  \n",
       "5          0.011511  \n",
       "6          0.011755  \n",
       "7          0.011927  \n",
       "8          0.012151  \n",
       "\n",
       "[9 rows x 39 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearch_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Предложите методику оценки того, какие признаки внесли наибольший вклад в модель (например, это могут быть веса в случае регрессии, а также большое количество моделей реализуют метод `feature_importances_` - оценка важности признаков). На основе предложенной методики проанализируйте, какие признаки внесли больший вклад в модель, а какие меньший?\n",
    "\n",
    "* Фунцкия на основе `feature_importances_` данных отборщика признаков возвращает признаки, ранжированные в порядке убывания важности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Var118', 'Var49', 'Var11', 'Var122', 'Var4', 'Var27', 'Var93',\n",
       "       'Var116', 'Var142', 'Var29', 'Var26', 'Var43', 'Var34', 'Var67',\n",
       "       'Var90', 'Var2', 'Var110', 'Var100', 'Var95', 'Var84', 'Var108',\n",
       "       'Var178', 'Var138', 'Var47', 'Var36', 'Var137', 'Var131', 'Var14',\n",
       "       'Var129', 'Var215'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Топ-30 наиболее важных признаков\n",
    "data_transformed_tree['feature_ranking'][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Var109', 'Var25', 'Var134', 'Var218', 'Var6', 'Var38', 'Var73',\n",
       "       'Var133', 'Var153', 'Var205', 'Var163', 'Var119', 'Var76', 'Var81',\n",
       "       'Var28', 'Var189', 'Var222', 'Var216', 'Var226', 'Var217', 'Var198',\n",
       "       'Var192', 'Var197', 'Var199', 'Var204', 'Var220', 'Var202', 'Var57',\n",
       "       'Var126', 'Var113'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Топ-30 наименее важных признаков\n",
    "data_transformed_tree['feature_ranking'][-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напоследок давайте посмотрим на объекты. На каких объектах достигается наибольшая ошибка классификации? Есть ли между этими объектами что-то общее? Видны ли какие-либо закономерности? Предположите, почему наибольшая ошибка достигается именно на этих объектах. В данном случае \"наибольшую\" ошибку можно понимать как отнесение объекта с чужому классу с большой долей уверенности (с высокой вероятностью).\n",
    "\n",
    "* Исходя из датафрейма Out[174], можно заметить, что объекты имеют схожие значения в признаках 64-69. Вероятно, такие комбинации признаков 64-69 наиболее характерны для класса \"отток\", поэтому классификатор на них ошибается с большой ошибкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=0.1, n_estimators=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': <40000x70 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2608048 stored elements in Compressed Sparse Row format>,\n",
       " 'cat_interval': (175, 212),\n",
       " 'feature_importances': array([  2.18784688e-04,   9.80160857e-05,   2.12297676e-04,\n",
       "          3.86411637e-05,   3.21916838e-04,   1.61548030e-02,\n",
       "          1.01320647e-02,   2.13217683e-04,   3.34947991e-04,\n",
       "          2.72048002e-05,   3.02683880e-04,   1.33302916e-02,\n",
       "          1.36316726e-04,   6.17350469e-04,   3.68970387e-04,\n",
       "          2.16897503e-04,   1.93768791e-04,   1.40148553e-02,\n",
       "          1.46823381e-02,   1.61987114e-04,   1.38295540e-02,\n",
       "          1.55930620e-02,   8.70270959e-05,   5.65179449e-05,\n",
       "          1.76139184e-02,   8.34585340e-05,   1.79723773e-04,\n",
       "          3.07237561e-04,   9.24580443e-05,   6.52517708e-03,\n",
       "          1.21462484e-04,   2.86678818e-04,   1.63686919e-02,\n",
       "          1.53643101e-04,   2.01741477e-04,   9.23867248e-05,\n",
       "          1.72339768e-03,   1.47580356e-04,   3.81172524e-04,\n",
       "          1.18031083e-04,   2.09960521e-05,   2.26232205e-04,\n",
       "          5.59733658e-03,   3.26211199e-04,   3.34808990e-04,\n",
       "          2.11363822e-04,   2.05467632e-02,   1.97965279e-04,\n",
       "          3.04162469e-04,   2.72400688e-04,   4.08002907e-04,\n",
       "          1.53083383e-04,   2.30744438e-04,   2.04891068e-04,\n",
       "          1.17199408e-02,   3.89640239e-04,   9.72764023e-05,\n",
       "          3.09217003e-04,   2.45116143e-04,   3.03982876e-04,\n",
       "          3.21195675e-04,   1.14135326e-02,   1.64025164e-02,\n",
       "          1.02160927e-02,   4.17247070e-04,   1.70197365e-02,\n",
       "          2.56328545e-04,   5.93304550e-03,   2.54743413e-04,\n",
       "          1.73008920e-02,   3.75698036e-04,   1.44209903e-02,\n",
       "          1.12034176e-04,   1.45159392e-02,   3.43967571e-04,\n",
       "          1.90238047e-04,   4.22357785e-04,   1.97682056e-04,\n",
       "          9.73902489e-05,   2.52914122e-04,   2.48229464e-04,\n",
       "          6.17283678e-05,   1.50775093e-02,   1.11224245e-04,\n",
       "          1.78585251e-04,   2.52860217e-04,   2.89411029e-04,\n",
       "          2.81951149e-04,   1.09269739e-04,   1.76976900e-04,\n",
       "          3.07963403e-04,   2.64978509e-04,   4.44623944e-04,\n",
       "          2.89610367e-04,   2.08059678e-04,   3.17941772e-04,\n",
       "          1.12759984e-04,   1.51868483e-02,   1.09218555e-04,\n",
       "          3.12724356e-04,   1.43919233e-02,   2.26080832e-02,\n",
       "          3.69018183e-04,   2.71498598e-04,   7.35297289e-05,\n",
       "          5.63847563e-04,   0.00000000e+00,   1.69942870e-02,\n",
       "          3.48045851e-04,   2.68428917e-04,   3.54937060e-05,\n",
       "          1.50245953e-02,   2.54847779e-04,   1.39503564e-02,\n",
       "          2.21009813e-02,   3.01402145e-04,   4.07954974e-04,\n",
       "          1.40869875e-04,   2.62585950e-04,   1.24111884e-04,\n",
       "          8.46857656e-03,   1.64363678e-02,   1.56384966e-02,\n",
       "          6.13202316e-04,   2.24283395e-04,   1.22303511e-04,\n",
       "          1.16773175e-04,   4.35850953e-04,   1.26186030e-02,\n",
       "          8.16462191e-05,   1.80771152e-03,   1.36918774e-02,\n",
       "          2.55352778e-04,   4.87705358e-04,   2.28743878e-04,\n",
       "          4.12372811e-04,   1.48215418e-02,   4.57661030e-04,\n",
       "          3.56533924e-04,   4.82328300e-04,   1.64734251e-02,\n",
       "          1.68475955e-04,   3.16152305e-04,   1.54487535e-04,\n",
       "          2.77378774e-04,   2.08051747e-04,   3.28416127e-04,\n",
       "          1.51486660e-02,   3.50748995e-04,   2.77854475e-04,\n",
       "          1.66107650e-02,   2.57099773e-04,   3.11170305e-04,\n",
       "          3.84871990e-04,   2.56533920e-04,   2.19439226e-04,\n",
       "          1.78257559e-04,   2.92634465e-04,   9.15489402e-04,\n",
       "          2.95740174e-04,   2.41388617e-04,   2.99527713e-04,\n",
       "          1.15843848e-04,   2.24078740e-04,   1.64312308e-04,\n",
       "          5.00119950e-03,   3.54981375e-04,   1.57752585e-04,\n",
       "          2.60797836e-04,   2.67666341e-04,   1.82605872e-04,\n",
       "          3.75883009e-04,   1.81409023e-02,   4.77533203e-04,\n",
       "          2.44175032e-04,   1.92541783e-02,   6.13185682e-03,\n",
       "          2.81731936e-03,   3.02546084e-03,   2.34591825e-03,\n",
       "          1.95708673e-02,   1.92124671e-02,   1.98846125e-02,\n",
       "          9.36563559e-03,   3.20846071e-03,   2.01060784e-02,\n",
       "          7.12223687e-03,   1.99063061e-02,   1.65897271e-02,\n",
       "          1.39201704e-02,   7.28222439e-03,   5.89721656e-03,\n",
       "          8.36905377e-03,   5.70301020e-03,   7.85198449e-03,\n",
       "          2.77160159e-04,   8.99597670e-03,   1.41221663e-04,\n",
       "          1.88145462e-02,   1.91040222e-02,   1.59414937e-02,\n",
       "          9.87282565e-03,   2.00994501e-02,   6.00017506e-03,\n",
       "          1.86396072e-02,   1.00497191e-02,   1.51925231e-04,\n",
       "          5.31701984e-03,   1.89785022e-02,   6.05268822e-03,\n",
       "          7.08616745e-03,   8.08118565e-03]),\n",
       " 'feature_ranking': Index(['Var118', 'Var49', 'Var11', 'Var122', 'Var4', 'Var27', 'Var93',\n",
       "        'Var116', 'Var142', 'Var29',\n",
       "        ...\n",
       "        'Var198', 'Var192', 'Var197', 'Var199', 'Var204', 'Var220', 'Var202',\n",
       "        'Var57', 'Var126', 'Var113'],\n",
       "       dtype='object', length=212),\n",
       " 'model': SelectFromModel(estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "         norm_order=1, prefit=True, threshold=None),\n",
       " 'num_interval': (0, 174)}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformed_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_hold_out, y_train, y_hold_out = train_test_split(X_resampled_LE, y_resampled_LE, \n",
    "                                                            test_size=0.2, random_state=123, \n",
    "                                                            stratify=y_resampled_LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=120,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_proba = gbc.predict_proba(X_hold_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_rate = pd.DataFrame(columns=['label', '                                                                                                   probability'])\n",
    "error_rate.label = y_hold_out\n",
    "error_rate.probability = class_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/placetexperiri/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.840975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.766646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.714476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.715289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.720987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.729546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.738385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.734569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.780573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.707796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.871993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.806394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.707468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.783958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.773338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.743963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.860980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.800934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.732443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  probability\n",
       "122      -1     0.840975\n",
       "125      -1     0.766646\n",
       "161      -1     0.714476\n",
       "306      -1     0.715289\n",
       "377      -1     0.720987\n",
       "408      -1     0.729546\n",
       "411      -1     0.738385\n",
       "489      -1     0.734569\n",
       "491      -1     0.780573\n",
       "528      -1     0.707796\n",
       "620      -1     0.871993\n",
       "639      -1     0.806394\n",
       "735      -1     0.707468\n",
       "856      -1     0.783958\n",
       "883      -1     0.773338\n",
       "1005     -1     0.743963\n",
       "1048     -1     0.860980\n",
       "1151     -1     0.800934\n",
       "1162     -1     0.732443"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate[error_rate.label==-1][error_rate.probability>0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/placetexperiri/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "wrong_classified = pd.DataFrame(X_hold_out[error_rate[error_rate.label==-1][error_rate.probability>0.7].index].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.456688</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.382639</td>\n",
       "      <td>-0.377555</td>\n",
       "      <td>-2.730890e-01</td>\n",
       "      <td>-0.478680</td>\n",
       "      <td>0.345459</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>-0.902594</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.246206</td>\n",
       "      <td>0.035066</td>\n",
       "      <td>-0.371421</td>\n",
       "      <td>-0.185516</td>\n",
       "      <td>-0.180612</td>\n",
       "      <td>-4.908111e-01</td>\n",
       "      <td>-0.359294</td>\n",
       "      <td>-2.075824</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>1.013851</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.260427</td>\n",
       "      <td>0.035066</td>\n",
       "      <td>-0.486074</td>\n",
       "      <td>-0.140026</td>\n",
       "      <td>-0.135164</td>\n",
       "      <td>-5.536680e-02</td>\n",
       "      <td>-0.080726</td>\n",
       "      <td>-2.198093</td>\n",
       "      <td>8.707446</td>\n",
       "      <td>0.906242</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>909.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.582716</td>\n",
       "      <td>0.035066</td>\n",
       "      <td>0.380195</td>\n",
       "      <td>2.612117</td>\n",
       "      <td>2.614459</td>\n",
       "      <td>3.863632e+00</td>\n",
       "      <td>2.704957</td>\n",
       "      <td>-0.981425</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>-0.864933</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3207.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2828.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.177941</td>\n",
       "      <td>0.035066</td>\n",
       "      <td>0.837216</td>\n",
       "      <td>-0.117281</td>\n",
       "      <td>-0.112439</td>\n",
       "      <td>1.623554e-01</td>\n",
       "      <td>-0.438885</td>\n",
       "      <td>-0.981425</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>2.907366</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.499354</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.185516</td>\n",
       "      <td>-0.180612</td>\n",
       "      <td>1.623554e-01</td>\n",
       "      <td>-0.359294</td>\n",
       "      <td>-2.198093</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>-0.893859</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1721.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.470910</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.215843</td>\n",
       "      <td>-0.210911</td>\n",
       "      <td>-5.536680e-02</td>\n",
       "      <td>-0.319498</td>\n",
       "      <td>0.674381</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>0.030957</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2644.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.516420</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.238588</td>\n",
       "      <td>-0.233635</td>\n",
       "      <td>-4.908111e-01</td>\n",
       "      <td>-0.359294</td>\n",
       "      <td>-2.039660</td>\n",
       "      <td>3.330258</td>\n",
       "      <td>-0.531814</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>898.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.121054</td>\n",
       "      <td>0.035066</td>\n",
       "      <td>-0.490852</td>\n",
       "      <td>-0.094536</td>\n",
       "      <td>-0.089715</td>\n",
       "      <td>-4.908111e-01</td>\n",
       "      <td>-0.160317</td>\n",
       "      <td>2.699579</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>0.665737</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.308782</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.443293</td>\n",
       "      <td>-0.438152</td>\n",
       "      <td>4.399307e-14</td>\n",
       "      <td>-0.478680</td>\n",
       "      <td>0.062172</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>-0.902594</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3352.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.035066</td>\n",
       "      <td>-0.140522</td>\n",
       "      <td>-0.079373</td>\n",
       "      <td>-0.074566</td>\n",
       "      <td>1.623554e-01</td>\n",
       "      <td>0.277434</td>\n",
       "      <td>-0.261584</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>-0.806892</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3811.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.340069</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.253751</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-4.908111e-01</td>\n",
       "      <td>-0.399089</td>\n",
       "      <td>0.620996</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>0.280240</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1455.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2986.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.126405</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.208261</td>\n",
       "      <td>-0.203336</td>\n",
       "      <td>-4.908111e-01</td>\n",
       "      <td>-0.160317</td>\n",
       "      <td>0.314461</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>0.732972</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.311626</td>\n",
       "      <td>0.035066</td>\n",
       "      <td>-0.400084</td>\n",
       "      <td>-0.193098</td>\n",
       "      <td>-0.188187</td>\n",
       "      <td>-4.908111e-01</td>\n",
       "      <td>-0.200112</td>\n",
       "      <td>0.314461</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>0.995418</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3149.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2411.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.519264</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.261333</td>\n",
       "      <td>-0.256359</td>\n",
       "      <td>-4.908111e-01</td>\n",
       "      <td>-0.359294</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>0.183721</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1823.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2469.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.064167</td>\n",
       "      <td>1.215889</td>\n",
       "      <td>0.311722</td>\n",
       "      <td>-0.064210</td>\n",
       "      <td>-0.059416</td>\n",
       "      <td>-2.730890e-01</td>\n",
       "      <td>0.078456</td>\n",
       "      <td>-0.981425</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>-0.901027</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.359980</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>0.784937</td>\n",
       "      <td>0.788952</td>\n",
       "      <td>1.033244e+00</td>\n",
       "      <td>1.033547</td>\n",
       "      <td>0.782013</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>1.145521</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3811.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.439622</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.124863</td>\n",
       "      <td>-0.120014</td>\n",
       "      <td>-5.536680e-02</td>\n",
       "      <td>0.476411</td>\n",
       "      <td>-0.621505</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>-0.882158</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.473754</td>\n",
       "      <td>-1.145757</td>\n",
       "      <td>-0.495629</td>\n",
       "      <td>-0.155189</td>\n",
       "      <td>-0.150313</td>\n",
       "      <td>-5.536680e-02</td>\n",
       "      <td>-0.319498</td>\n",
       "      <td>0.466006</td>\n",
       "      <td>-0.254534</td>\n",
       "      <td>1.115632</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>977.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4             5         6   \\\n",
       "0  -0.456688 -1.145757 -0.495629 -0.382639 -0.377555 -2.730890e-01 -0.478680   \n",
       "1  -0.246206  0.035066 -0.371421 -0.185516 -0.180612 -4.908111e-01 -0.359294   \n",
       "2  -0.260427  0.035066 -0.486074 -0.140026 -0.135164 -5.536680e-02 -0.080726   \n",
       "3   1.582716  0.035066  0.380195  2.612117  2.614459  3.863632e+00  2.704957   \n",
       "4  -0.177941  0.035066  0.837216 -0.117281 -0.112439  1.623554e-01 -0.438885   \n",
       "5  -0.499354 -1.145757 -0.495629 -0.185516 -0.180612  1.623554e-01 -0.359294   \n",
       "6  -0.470910 -1.145757 -0.495629 -0.215843 -0.210911 -5.536680e-02 -0.319498   \n",
       "7  -0.516420 -1.145757 -0.495629 -0.238588 -0.233635 -4.908111e-01 -0.359294   \n",
       "8  -0.121054  0.035066 -0.490852 -0.094536 -0.089715 -4.908111e-01 -0.160317   \n",
       "9  -0.308782 -1.145757 -0.495629 -0.443293 -0.438152  4.399307e-14 -0.478680   \n",
       "10  0.001253  0.035066 -0.140522 -0.079373 -0.074566  1.623554e-01  0.277434   \n",
       "11 -0.340069 -1.145757 -0.495629 -0.253751 -0.248784 -4.908111e-01 -0.399089   \n",
       "12  0.126405 -1.145757 -0.495629 -0.208261 -0.203336 -4.908111e-01 -0.160317   \n",
       "13 -0.311626  0.035066 -0.400084 -0.193098 -0.188187 -4.908111e-01 -0.200112   \n",
       "14 -0.519264 -1.145757 -0.495629 -0.261333 -0.256359 -4.908111e-01 -0.359294   \n",
       "15 -0.064167  1.215889  0.311722 -0.064210 -0.059416 -2.730890e-01  0.078456   \n",
       "16 -0.359980 -1.145757 -0.495629  0.784937  0.788952  1.033244e+00  1.033547   \n",
       "17 -0.439622 -1.145757 -0.495629 -0.124863 -0.120014 -5.536680e-02  0.476411   \n",
       "18 -0.473754 -1.145757 -0.495629 -0.155189 -0.150313 -5.536680e-02 -0.319498   \n",
       "\n",
       "          7         8         9  ...     60      61   62      63   64   65  \\\n",
       "0   0.345459 -0.254534 -0.902594 ...   20.0  3088.0  4.0  1300.0  4.0  2.0   \n",
       "1  -2.075824 -0.254534  1.013851 ...   11.0   537.0  4.0   114.0  0.0  2.0   \n",
       "2  -2.198093  8.707446  0.906242 ...   11.0   783.0  4.0   909.0  0.0  2.0   \n",
       "3  -0.981425 -0.254534 -0.864933 ...    6.0  3207.0  3.0  2828.0  3.0  2.0   \n",
       "4  -0.981425 -0.254534  2.907366 ...   20.0   328.0  6.0  2498.0  4.0  2.0   \n",
       "5  -2.198093 -0.254534 -0.893859 ...   20.0  1721.0  4.0   653.0  4.0  2.0   \n",
       "6   0.674381 -0.254534  0.030957 ...   11.0  2644.0  4.0   634.0  0.0  2.0   \n",
       "7  -2.039660  3.330258 -0.531814 ...   11.0   898.0  4.0   438.0  0.0  2.0   \n",
       "8   2.699579 -0.254534  0.665737 ...   11.0   743.0  4.0   234.0  0.0  2.0   \n",
       "9   0.062172 -0.254534 -0.902594 ...   11.0  3352.0  4.0  2502.0  0.0  2.0   \n",
       "10 -0.261584 -0.254534 -0.806892 ...   11.0  3496.0  0.0  3811.0  0.0  1.0   \n",
       "11  0.620996 -0.254534  0.280240 ...   11.0  1455.0  4.0  2986.0  0.0  2.0   \n",
       "12  0.314461 -0.254534  0.732972 ...   11.0   229.0  4.0  1506.0  0.0  2.0   \n",
       "13  0.314461 -0.254534  0.995418 ...   11.0  3149.0  4.0  2411.0  3.0  2.0   \n",
       "14  0.559000 -0.254534  0.183721 ...   20.0  1823.0  5.0  2469.0  4.0  2.0   \n",
       "15 -0.981425 -0.254534 -0.901027 ...   11.0   328.0  4.0  2498.0  0.0  2.0   \n",
       "16  0.782013 -0.254534  1.145521 ...   11.0  3811.0  4.0   192.0  0.0  2.0   \n",
       "17 -0.621505 -0.254534 -0.882158 ...   11.0   470.0  3.0   223.0  0.0  2.0   \n",
       "18  0.466006 -0.254534  1.115632 ...   11.0   977.0  4.0  2850.0  0.0  2.0   \n",
       "\n",
       "      66   67    68   69  \n",
       "0    7.0  2.0   8.0  4.0  \n",
       "1    2.0  2.0   8.0  4.0  \n",
       "2   18.0  2.0   8.0  4.0  \n",
       "3    1.0  4.0  25.0  1.0  \n",
       "4    7.0  3.0  15.0  4.0  \n",
       "5    7.0  2.0   8.0  4.0  \n",
       "6    2.0  2.0   8.0  4.0  \n",
       "7    7.0  2.0   8.0  4.0  \n",
       "8    7.0  2.0   8.0  4.0  \n",
       "9    2.0  2.0   8.0  4.0  \n",
       "10   7.0  0.0  26.0  0.0  \n",
       "11   4.0  2.0   8.0  4.0  \n",
       "12  12.0  2.0   8.0  4.0  \n",
       "13  12.0  2.0   8.0  4.0  \n",
       "14  20.0  0.0   8.0  4.0  \n",
       "15  10.0  3.0  14.0  0.0  \n",
       "16  12.0  2.0   8.0  4.0  \n",
       "17  20.0  0.0   8.0  4.0  \n",
       "18   7.0  2.0   8.0  4.0  \n",
       "\n",
       "[19 rows x 70 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. По итогам проведенных экспериментов постройте финальную решение - модель с наилучшим качеством. Укажите, какие преобразования данных, параметры и пр. вы выбрали для построения финальной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Итоговая модель имеет следующие параметры:\n",
    "* Заполнение средним значением пропущенных числовых данных и последующая нормализация;\n",
    "* Заполнение пропущенных категориальных данных 'unknown' и использование LabelEncoding()\n",
    "* Отбор признаков с помощью tree-based feature selection model\n",
    "* Использование NearMiss UnderSampling model\n",
    "* n_estimators=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping the null columns...\n",
      "Dropped num-type column: Var8\n",
      "Dropped num-type column: Var15\n",
      "Dropped num-type column: Var20\n",
      "Dropped num-type column: Var31\n",
      "Dropped num-type column: Var32\n",
      "Dropped num-type column: Var39\n",
      "Dropped num-type column: Var42\n",
      "Dropped num-type column: Var48\n",
      "Dropped num-type column: Var52\n",
      "Dropped num-type column: Var55\n",
      "Dropped num-type column: Var79\n",
      "Dropped num-type column: Var141\n",
      "Dropped num-type column: Var167\n",
      "Dropped num-type column: Var169\n",
      "Dropped num-type column: Var175\n",
      "Dropped num-type column: Var185\n",
      "Dropped cal-type column: Var209\n",
      "Dropped cal-type column: Var230\n",
      "Dropped 16 num-type cols and 2 cat-types cols\n",
      "Num-type feature have the indices from 0 up to 174.\n",
      "Cal-type feature have the indices from 175 up to 212.\n",
      "Splitting the numeric and categorical columns...\n",
      "Processing the numeric columns...\n",
      "Filling the absent values with the columns' means...\n",
      "Normalizing the data...\n",
      "Processing the categorical columns...\n",
      "Filling the absent categorical values...\n",
      "Applying a LabelEncoder to the categorical columns...\n",
      "Joining the numerical and categorical columns...\n",
      "Appling the tree-based feature selection method\n",
      "New shape of X-data is (40000, 70)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('orange_small_churn_data.txt', header=0, sep=',')\n",
    "\n",
    "data_result_model = transform_data(data=data, y=y, feature_selection='tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "near_miss = NearMiss(random_state=0)\n",
    "X_resampled_result, y_resampled_result = near_miss.fit_sample(data_result_model['X'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=0.1, n_estimators=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating scores...\n",
      "Done!\n",
      "Calculating scores...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(X_resampled_result, y_resampled_result, gbc, 'final model', scores_cv)\n",
    "gbc.fit(X_resampled_result, y_resampled_result)\n",
    "get_scores_hold_out(X_resampled_result, y_resampled_result, gbc, 'final model', scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision test</th>\n",
       "      <th>precision train</th>\n",
       "      <th>recall test</th>\n",
       "      <th>recall train</th>\n",
       "      <th>f1-score test</th>\n",
       "      <th>f1-score train</th>\n",
       "      <th>PR-score test</th>\n",
       "      <th>PR-score train</th>\n",
       "      <th>ROC AUC test</th>\n",
       "      <th>ROC AUC train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.543336</td>\n",
       "      <td>0.802362</td>\n",
       "      <td>0.0302397</td>\n",
       "      <td>0.048051</td>\n",
       "      <td>0.0572347</td>\n",
       "      <td>0.0905694</td>\n",
       "      <td>0.267859</td>\n",
       "      <td>0.37886</td>\n",
       "      <td>0.736245</td>\n",
       "      <td>0.7939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.580892</td>\n",
       "      <td>0.7348</td>\n",
       "      <td>0.13273</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.215669</td>\n",
       "      <td>0.277421</td>\n",
       "      <td>0.432115</td>\n",
       "      <td>0.555641</td>\n",
       "      <td>0.734751</td>\n",
       "      <td>0.808143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.735712</td>\n",
       "      <td>0.346785</td>\n",
       "      <td>0.410786</td>\n",
       "      <td>0.45063</td>\n",
       "      <td>0.52707</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.693886</td>\n",
       "      <td>0.738533</td>\n",
       "      <td>0.81657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.651932</td>\n",
       "      <td>0.738259</td>\n",
       "      <td>0.52521</td>\n",
       "      <td>0.60988</td>\n",
       "      <td>0.581503</td>\n",
       "      <td>0.667917</td>\n",
       "      <td>0.658956</td>\n",
       "      <td>0.770807</td>\n",
       "      <td>0.735974</td>\n",
       "      <td>0.82293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.664977</td>\n",
       "      <td>0.740377</td>\n",
       "      <td>0.639449</td>\n",
       "      <td>0.711441</td>\n",
       "      <td>0.651841</td>\n",
       "      <td>0.725551</td>\n",
       "      <td>0.705328</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.736613</td>\n",
       "      <td>0.830574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.670118</td>\n",
       "      <td>0.74205</td>\n",
       "      <td>0.718073</td>\n",
       "      <td>0.793011</td>\n",
       "      <td>0.693176</td>\n",
       "      <td>0.766639</td>\n",
       "      <td>0.733713</td>\n",
       "      <td>0.839188</td>\n",
       "      <td>0.732077</td>\n",
       "      <td>0.835514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.667147</td>\n",
       "      <td>0.734895</td>\n",
       "      <td>0.694553</td>\n",
       "      <td>0.767053</td>\n",
       "      <td>0.68044</td>\n",
       "      <td>0.750612</td>\n",
       "      <td>0.724784</td>\n",
       "      <td>0.823841</td>\n",
       "      <td>0.735272</td>\n",
       "      <td>0.830512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.824846</td>\n",
       "      <td>0.897813</td>\n",
       "      <td>0.67541</td>\n",
       "      <td>0.743784</td>\n",
       "      <td>0.742652</td>\n",
       "      <td>0.813564</td>\n",
       "      <td>0.877452</td>\n",
       "      <td>0.929137</td>\n",
       "      <td>0.852617</td>\n",
       "      <td>0.915934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.686542</td>\n",
       "      <td>0.767378</td>\n",
       "      <td>0.257055</td>\n",
       "      <td>0.291751</td>\n",
       "      <td>0.373709</td>\n",
       "      <td>0.422743</td>\n",
       "      <td>0.512349</td>\n",
       "      <td>0.598667</td>\n",
       "      <td>0.816627</td>\n",
       "      <td>0.858223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means + normalization</th>\n",
       "      <td>0.670491</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717065</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692881</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733191</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.731737</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means, no normalization</th>\n",
       "      <td>0.669585</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717064</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692417</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733693</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.73207</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with medians</th>\n",
       "      <td>0.653652</td>\n",
       "      <td>0.726114</td>\n",
       "      <td>0.725471</td>\n",
       "      <td>0.80788</td>\n",
       "      <td>0.687651</td>\n",
       "      <td>0.764801</td>\n",
       "      <td>0.724906</td>\n",
       "      <td>0.83455</td>\n",
       "      <td>0.723056</td>\n",
       "      <td>0.83121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with the most frequent</th>\n",
       "      <td>0.654692</td>\n",
       "      <td>0.729067</td>\n",
       "      <td>0.726476</td>\n",
       "      <td>0.804772</td>\n",
       "      <td>0.688676</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.726293</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.724057</td>\n",
       "      <td>0.832352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder()</th>\n",
       "      <td>0.670007</td>\n",
       "      <td>0.741787</td>\n",
       "      <td>0.717065</td>\n",
       "      <td>0.793095</td>\n",
       "      <td>0.692641</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>0.733419</td>\n",
       "      <td>0.839055</td>\n",
       "      <td>0.731933</td>\n",
       "      <td>0.83515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder() + OneHotEncoder()</th>\n",
       "      <td>0.674134</td>\n",
       "      <td>0.733599</td>\n",
       "      <td>0.718411</td>\n",
       "      <td>0.78419</td>\n",
       "      <td>0.695426</td>\n",
       "      <td>0.758025</td>\n",
       "      <td>0.739827</td>\n",
       "      <td>0.829013</td>\n",
       "      <td>0.73708</td>\n",
       "      <td>0.82366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + l1-based feature selection</th>\n",
       "      <td>0.671252</td>\n",
       "      <td>0.730653</td>\n",
       "      <td>0.723117</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.696023</td>\n",
       "      <td>0.755079</td>\n",
       "      <td>0.737412</td>\n",
       "      <td>0.82327</td>\n",
       "      <td>0.735509</td>\n",
       "      <td>0.821125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + tree-based feature selection</th>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.746206</td>\n",
       "      <td>0.706656</td>\n",
       "      <td>0.794271</td>\n",
       "      <td>0.687401</td>\n",
       "      <td>0.769463</td>\n",
       "      <td>0.733625</td>\n",
       "      <td>0.845065</td>\n",
       "      <td>0.731626</td>\n",
       "      <td>0.839669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + 20 selected correlation-based features</th>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.73285</td>\n",
       "      <td>0.705307</td>\n",
       "      <td>0.775285</td>\n",
       "      <td>0.687385</td>\n",
       "      <td>0.753432</td>\n",
       "      <td>0.735469</td>\n",
       "      <td>0.823183</td>\n",
       "      <td>0.731638</td>\n",
       "      <td>0.819441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52+ OneHotEncoder() + l1-based feature selection</th>\n",
       "      <td>0.674474</td>\n",
       "      <td>0.725556</td>\n",
       "      <td>0.717736</td>\n",
       "      <td>0.773857</td>\n",
       "      <td>0.695202</td>\n",
       "      <td>0.748912</td>\n",
       "      <td>0.737743</td>\n",
       "      <td>0.810676</td>\n",
       "      <td>0.737681</td>\n",
       "      <td>0.811592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + OneHotEncoder() + tree-based feature selection</th>\n",
       "      <td>0.675147</td>\n",
       "      <td>0.73451</td>\n",
       "      <td>0.713706</td>\n",
       "      <td>0.786038</td>\n",
       "      <td>0.693682</td>\n",
       "      <td>0.759374</td>\n",
       "      <td>0.739823</td>\n",
       "      <td>0.828341</td>\n",
       "      <td>0.737301</td>\n",
       "      <td>0.823631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final model</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.906011</td>\n",
       "      <td>0.679779</td>\n",
       "      <td>0.760333</td>\n",
       "      <td>0.742295</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.878046</td>\n",
       "      <td>0.935795</td>\n",
       "      <td>0.853239</td>\n",
       "      <td>0.922854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision test  \\\n",
       "Positive class fraction = 0.11                           0.543336   \n",
       "Positive class fraction = 0.21                           0.580892   \n",
       "Positive class fraction = 0.33                           0.645367   \n",
       "Positive class fraction = 0.42                           0.651932   \n",
       "Positive class fraction = 0.47                           0.664977   \n",
       "Positive class fraction = 0.52                           0.670118   \n",
       "RandomUnderSampler                                       0.667147   \n",
       "NearMiss UnderSampler                                    0.824846   \n",
       "InstanceHardnessThreshold UnderSampler                   0.686542   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.670491   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.669585   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.653652   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.654692   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.670007   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.674134   \n",
       "Positive class fraction = 0.52 + l1-based featu...       0.671252   \n",
       "Positive class fraction = 0.52 + tree-based fea...       0.669753   \n",
       "Positive class fraction = 0.52 + 20 selected co...       0.670752   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...       0.674474   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...       0.675147   \n",
       "final model                                              0.817647   \n",
       "\n",
       "                                                   precision train  \\\n",
       "Positive class fraction = 0.11                            0.802362   \n",
       "Positive class fraction = 0.21                              0.7348   \n",
       "Positive class fraction = 0.33                            0.735712   \n",
       "Positive class fraction = 0.42                            0.738259   \n",
       "Positive class fraction = 0.47                            0.740377   \n",
       "Positive class fraction = 0.52                             0.74205   \n",
       "RandomUnderSampler                                        0.734895   \n",
       "NearMiss UnderSampler                                     0.897813   \n",
       "InstanceHardnessThreshold UnderSampler                    0.767378   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.741787   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.741787   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.726114   \n",
       "Positive class fraction = 0.52 fill NaNs with t...        0.729067   \n",
       "Positive class fraction = 0.52 + LableEncoder()           0.741787   \n",
       "Positive class fraction = 0.52 + LableEncoder()...        0.733599   \n",
       "Positive class fraction = 0.52 + l1-based featu...        0.730653   \n",
       "Positive class fraction = 0.52 + tree-based fea...        0.746206   \n",
       "Positive class fraction = 0.52 + 20 selected co...         0.73285   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...        0.725556   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...         0.73451   \n",
       "final model                                               0.906011   \n",
       "\n",
       "                                                   recall test recall train  \\\n",
       "Positive class fraction = 0.11                       0.0302397     0.048051   \n",
       "Positive class fraction = 0.21                         0.13273     0.171034   \n",
       "Positive class fraction = 0.33                        0.346785     0.410786   \n",
       "Positive class fraction = 0.42                         0.52521      0.60988   \n",
       "Positive class fraction = 0.47                        0.639449     0.711441   \n",
       "Positive class fraction = 0.52                        0.718073     0.793011   \n",
       "RandomUnderSampler                                    0.694553     0.767053   \n",
       "NearMiss UnderSampler                                  0.67541     0.743784   \n",
       "InstanceHardnessThreshold UnderSampler                0.257055     0.291751   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.717065     0.793095   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.717064     0.793095   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.725471      0.80788   \n",
       "Positive class fraction = 0.52 fill NaNs with t...    0.726476     0.804772   \n",
       "Positive class fraction = 0.52 + LableEncoder()       0.717065     0.793095   \n",
       "Positive class fraction = 0.52 + LableEncoder()...    0.718411      0.78419   \n",
       "Positive class fraction = 0.52 + l1-based featu...    0.723117      0.78125   \n",
       "Positive class fraction = 0.52 + tree-based fea...    0.706656     0.794271   \n",
       "Positive class fraction = 0.52 + 20 selected co...    0.705307     0.775285   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...    0.717736     0.773857   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...    0.713706     0.786038   \n",
       "final model                                           0.679779     0.760333   \n",
       "\n",
       "                                                   f1-score test  \\\n",
       "Positive class fraction = 0.11                         0.0572347   \n",
       "Positive class fraction = 0.21                          0.215669   \n",
       "Positive class fraction = 0.33                           0.45063   \n",
       "Positive class fraction = 0.42                          0.581503   \n",
       "Positive class fraction = 0.47                          0.651841   \n",
       "Positive class fraction = 0.52                          0.693176   \n",
       "RandomUnderSampler                                       0.68044   \n",
       "NearMiss UnderSampler                                   0.742652   \n",
       "InstanceHardnessThreshold UnderSampler                  0.373709   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.692881   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.692417   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.687651   \n",
       "Positive class fraction = 0.52 fill NaNs with t...      0.688676   \n",
       "Positive class fraction = 0.52 + LableEncoder()         0.692641   \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.695426   \n",
       "Positive class fraction = 0.52 + l1-based featu...      0.696023   \n",
       "Positive class fraction = 0.52 + tree-based fea...      0.687401   \n",
       "Positive class fraction = 0.52 + 20 selected co...      0.687385   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...      0.695202   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...      0.693682   \n",
       "final model                                             0.742295   \n",
       "\n",
       "                                                   f1-score train  \\\n",
       "Positive class fraction = 0.11                          0.0905694   \n",
       "Positive class fraction = 0.21                           0.277421   \n",
       "Positive class fraction = 0.33                            0.52707   \n",
       "Positive class fraction = 0.42                           0.667917   \n",
       "Positive class fraction = 0.47                           0.725551   \n",
       "Positive class fraction = 0.52                           0.766639   \n",
       "RandomUnderSampler                                       0.750612   \n",
       "NearMiss UnderSampler                                    0.813564   \n",
       "InstanceHardnessThreshold UnderSampler                   0.422743   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.766533   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.766533   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.764801   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.765019   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.766533   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.758025   \n",
       "Positive class fraction = 0.52 + l1-based featu...       0.755079   \n",
       "Positive class fraction = 0.52 + tree-based fea...       0.769463   \n",
       "Positive class fraction = 0.52 + 20 selected co...       0.753432   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...       0.748912   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...       0.759374   \n",
       "final model                                                0.8268   \n",
       "\n",
       "                                                   PR-score test  \\\n",
       "Positive class fraction = 0.11                          0.267859   \n",
       "Positive class fraction = 0.21                          0.432115   \n",
       "Positive class fraction = 0.33                          0.580529   \n",
       "Positive class fraction = 0.42                          0.658956   \n",
       "Positive class fraction = 0.47                          0.705328   \n",
       "Positive class fraction = 0.52                          0.733713   \n",
       "RandomUnderSampler                                      0.724784   \n",
       "NearMiss UnderSampler                                   0.877452   \n",
       "InstanceHardnessThreshold UnderSampler                  0.512349   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.733191   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.733693   \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.724906   \n",
       "Positive class fraction = 0.52 fill NaNs with t...      0.726293   \n",
       "Positive class fraction = 0.52 + LableEncoder()         0.733419   \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.739827   \n",
       "Positive class fraction = 0.52 + l1-based featu...      0.737412   \n",
       "Positive class fraction = 0.52 + tree-based fea...      0.733625   \n",
       "Positive class fraction = 0.52 + 20 selected co...      0.735469   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...      0.737743   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...      0.739823   \n",
       "final model                                             0.878046   \n",
       "\n",
       "                                                   PR-score train  \\\n",
       "Positive class fraction = 0.11                            0.37886   \n",
       "Positive class fraction = 0.21                           0.555641   \n",
       "Positive class fraction = 0.33                           0.693886   \n",
       "Positive class fraction = 0.42                           0.770807   \n",
       "Positive class fraction = 0.47                           0.811321   \n",
       "Positive class fraction = 0.52                           0.839188   \n",
       "RandomUnderSampler                                       0.823841   \n",
       "NearMiss UnderSampler                                    0.929137   \n",
       "InstanceHardnessThreshold UnderSampler                   0.598667   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.839055   \n",
       "Positive class fraction = 0.52 fill NaNs with m...       0.839055   \n",
       "Positive class fraction = 0.52 fill NaNs with m...        0.83455   \n",
       "Positive class fraction = 0.52 fill NaNs with t...       0.835821   \n",
       "Positive class fraction = 0.52 + LableEncoder()          0.839055   \n",
       "Positive class fraction = 0.52 + LableEncoder()...       0.829013   \n",
       "Positive class fraction = 0.52 + l1-based featu...        0.82327   \n",
       "Positive class fraction = 0.52 + tree-based fea...       0.845065   \n",
       "Positive class fraction = 0.52 + 20 selected co...       0.823183   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...       0.810676   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...       0.828341   \n",
       "final model                                              0.935795   \n",
       "\n",
       "                                                   ROC AUC test ROC AUC train  \n",
       "Positive class fraction = 0.11                         0.736245        0.7939  \n",
       "Positive class fraction = 0.21                         0.734751      0.808143  \n",
       "Positive class fraction = 0.33                         0.738533       0.81657  \n",
       "Positive class fraction = 0.42                         0.735974       0.82293  \n",
       "Positive class fraction = 0.47                         0.736613      0.830574  \n",
       "Positive class fraction = 0.52                         0.732077      0.835514  \n",
       "RandomUnderSampler                                     0.735272      0.830512  \n",
       "NearMiss UnderSampler                                  0.852617      0.915934  \n",
       "InstanceHardnessThreshold UnderSampler                 0.816627      0.858223  \n",
       "Positive class fraction = 0.52 fill NaNs with m...     0.731737       0.83515  \n",
       "Positive class fraction = 0.52 fill NaNs with m...      0.73207       0.83515  \n",
       "Positive class fraction = 0.52 fill NaNs with m...     0.723056       0.83121  \n",
       "Positive class fraction = 0.52 fill NaNs with t...     0.724057      0.832352  \n",
       "Positive class fraction = 0.52 + LableEncoder()        0.731933       0.83515  \n",
       "Positive class fraction = 0.52 + LableEncoder()...      0.73708       0.82366  \n",
       "Positive class fraction = 0.52 + l1-based featu...     0.735509      0.821125  \n",
       "Positive class fraction = 0.52 + tree-based fea...     0.731626      0.839669  \n",
       "Positive class fraction = 0.52 + 20 selected co...     0.731638      0.819441  \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...     0.737681      0.811592  \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...     0.737301      0.823631  \n",
       "final model                                            0.853239      0.922854  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>PR-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.11</th>\n",
       "      <td>0.766467</td>\n",
       "      <td>0.0430108</td>\n",
       "      <td>0.0814508</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.784093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.21</th>\n",
       "      <td>0.70901</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>0.262798</td>\n",
       "      <td>0.533307</td>\n",
       "      <td>0.797007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.33</th>\n",
       "      <td>0.721351</td>\n",
       "      <td>0.401882</td>\n",
       "      <td>0.516185</td>\n",
       "      <td>0.678596</td>\n",
       "      <td>0.806856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.42</th>\n",
       "      <td>0.72439</td>\n",
       "      <td>0.59879</td>\n",
       "      <td>0.655629</td>\n",
       "      <td>0.754367</td>\n",
       "      <td>0.812276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.47</th>\n",
       "      <td>0.728933</td>\n",
       "      <td>0.697581</td>\n",
       "      <td>0.712912</td>\n",
       "      <td>0.796059</td>\n",
       "      <td>0.817358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomUnderSampler</th>\n",
       "      <td>0.721953</td>\n",
       "      <td>0.750336</td>\n",
       "      <td>0.735871</td>\n",
       "      <td>0.810379</td>\n",
       "      <td>0.817704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearMiss UnderSampler</th>\n",
       "      <td>0.884412</td>\n",
       "      <td>0.730175</td>\n",
       "      <td>0.799926</td>\n",
       "      <td>0.921433</td>\n",
       "      <td>0.906948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InstanceHardnessThreshold UnderSampler</th>\n",
       "      <td>0.755157</td>\n",
       "      <td>0.28293</td>\n",
       "      <td>0.411635</td>\n",
       "      <td>0.588451</td>\n",
       "      <td>0.853209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means + normalization</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with means, no normalization</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with medians</th>\n",
       "      <td>0.716192</td>\n",
       "      <td>0.793683</td>\n",
       "      <td>0.752949</td>\n",
       "      <td>0.823079</td>\n",
       "      <td>0.817187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 fill NaNs with the most frequent</th>\n",
       "      <td>0.716924</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.75426</td>\n",
       "      <td>0.821945</td>\n",
       "      <td>0.817341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder()</th>\n",
       "      <td>0.731923</td>\n",
       "      <td>0.778898</td>\n",
       "      <td>0.75468</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.822084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + LableEncoder() + OneHotEncoder()</th>\n",
       "      <td>0.731396</td>\n",
       "      <td>0.769489</td>\n",
       "      <td>0.749959</td>\n",
       "      <td>0.814671</td>\n",
       "      <td>0.811156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + l1-based feature selection</th>\n",
       "      <td>0.725167</td>\n",
       "      <td>0.767809</td>\n",
       "      <td>0.745879</td>\n",
       "      <td>0.812827</td>\n",
       "      <td>0.809617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + tree-based feature selection</th>\n",
       "      <td>0.73746</td>\n",
       "      <td>0.780578</td>\n",
       "      <td>0.758407</td>\n",
       "      <td>0.832506</td>\n",
       "      <td>0.825036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + 20 selected correlation-based features</th>\n",
       "      <td>0.727157</td>\n",
       "      <td>0.764785</td>\n",
       "      <td>0.745496</td>\n",
       "      <td>0.811537</td>\n",
       "      <td>0.806987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52+ OneHotEncoder() + l1-based feature selection</th>\n",
       "      <td>0.719539</td>\n",
       "      <td>0.756048</td>\n",
       "      <td>0.737342</td>\n",
       "      <td>0.799162</td>\n",
       "      <td>0.801018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive class fraction = 0.52 + OneHotEncoder() + tree-based feature selection</th>\n",
       "      <td>0.729213</td>\n",
       "      <td>0.769153</td>\n",
       "      <td>0.748651</td>\n",
       "      <td>0.816683</td>\n",
       "      <td>0.813735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final model</th>\n",
       "      <td>0.895396</td>\n",
       "      <td>0.74496</td>\n",
       "      <td>0.81328</td>\n",
       "      <td>0.927849</td>\n",
       "      <td>0.912938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   precision     recall  \\\n",
       "Positive class fraction = 0.11                      0.766467  0.0430108   \n",
       "Positive class fraction = 0.21                       0.70901    0.16129   \n",
       "Positive class fraction = 0.33                      0.721351   0.401882   \n",
       "Positive class fraction = 0.42                       0.72439    0.59879   \n",
       "Positive class fraction = 0.47                      0.728933   0.697581   \n",
       "Positive class fraction = 0.52                      0.731923   0.778898   \n",
       "RandomUnderSampler                                  0.721953   0.750336   \n",
       "NearMiss UnderSampler                               0.884412   0.730175   \n",
       "InstanceHardnessThreshold UnderSampler              0.755157    0.28293   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.731923   0.778898   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.731923   0.778898   \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.716192   0.793683   \n",
       "Positive class fraction = 0.52 fill NaNs with t...  0.716924   0.795699   \n",
       "Positive class fraction = 0.52 + LableEncoder()     0.731923   0.778898   \n",
       "Positive class fraction = 0.52 + LableEncoder()...  0.731396   0.769489   \n",
       "Positive class fraction = 0.52 + l1-based featu...  0.725167   0.767809   \n",
       "Positive class fraction = 0.52 + tree-based fea...   0.73746   0.780578   \n",
       "Positive class fraction = 0.52 + 20 selected co...  0.727157   0.764785   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...  0.719539   0.756048   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...  0.729213   0.769153   \n",
       "final model                                         0.895396    0.74496   \n",
       "\n",
       "                                                     f1-score  PR-score  \\\n",
       "Positive class fraction = 0.11                      0.0814508  0.356719   \n",
       "Positive class fraction = 0.21                       0.262798  0.533307   \n",
       "Positive class fraction = 0.33                       0.516185  0.678596   \n",
       "Positive class fraction = 0.42                       0.655629  0.754367   \n",
       "Positive class fraction = 0.47                       0.712912  0.796059   \n",
       "Positive class fraction = 0.52                        0.75468  0.827556   \n",
       "RandomUnderSampler                                   0.735871  0.810379   \n",
       "NearMiss UnderSampler                                0.799926  0.921433   \n",
       "InstanceHardnessThreshold UnderSampler               0.411635  0.588451   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.75468  0.827556   \n",
       "Positive class fraction = 0.52 fill NaNs with m...    0.75468  0.827556   \n",
       "Positive class fraction = 0.52 fill NaNs with m...   0.752949  0.823079   \n",
       "Positive class fraction = 0.52 fill NaNs with t...    0.75426  0.821945   \n",
       "Positive class fraction = 0.52 + LableEncoder()       0.75468  0.827556   \n",
       "Positive class fraction = 0.52 + LableEncoder()...   0.749959  0.814671   \n",
       "Positive class fraction = 0.52 + l1-based featu...   0.745879  0.812827   \n",
       "Positive class fraction = 0.52 + tree-based fea...   0.758407  0.832506   \n",
       "Positive class fraction = 0.52 + 20 selected co...   0.745496  0.811537   \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...   0.737342  0.799162   \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...   0.748651  0.816683   \n",
       "final model                                           0.81328  0.927849   \n",
       "\n",
       "                                                     ROC AUC  \n",
       "Positive class fraction = 0.11                      0.784093  \n",
       "Positive class fraction = 0.21                      0.797007  \n",
       "Positive class fraction = 0.33                      0.806856  \n",
       "Positive class fraction = 0.42                      0.812276  \n",
       "Positive class fraction = 0.47                      0.817358  \n",
       "Positive class fraction = 0.52                      0.822084  \n",
       "RandomUnderSampler                                  0.817704  \n",
       "NearMiss UnderSampler                               0.906948  \n",
       "InstanceHardnessThreshold UnderSampler              0.853209  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.822084  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.822084  \n",
       "Positive class fraction = 0.52 fill NaNs with m...  0.817187  \n",
       "Positive class fraction = 0.52 fill NaNs with t...  0.817341  \n",
       "Positive class fraction = 0.52 + LableEncoder()     0.822084  \n",
       "Positive class fraction = 0.52 + LableEncoder()...  0.811156  \n",
       "Positive class fraction = 0.52 + l1-based featu...  0.809617  \n",
       "Positive class fraction = 0.52 + tree-based fea...  0.825036  \n",
       "Positive class fraction = 0.52 + 20 selected co...  0.806987  \n",
       "Positive class fraction = 0.52+ OneHotEncoder()...  0.801018  \n",
       "Positive class fraction = 0.52 + OneHotEncoder(...  0.813735  \n",
       "final model                                         0.912938  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Подумайте, можно ли еще улучшить модель? Что для этого можно сделать? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Пресказывать пропущенных значений числовых и категориальных признаков, а не заполнять их средним значением или 'unknown'\n",
    "* Использовать OverSampling вместо UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
